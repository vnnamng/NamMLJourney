{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b21851e",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6db03b82-98d8-40dd-ae5f-96b4f256536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTPUT_FOLDER = '../model/'\n",
    "\n",
    "seed = 88\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e6de5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "# Read the CSV file with specified column names\n",
    "df = pd.read_csv(\"../dataset/training.1600000.processed.noemoticon.csv\", \n",
    "                 encoding=\"ISO-8859-1\", names=column_names)\n",
    "\n",
    "def reduce_sample(df, frac, random_state):\n",
    "    df = df.sample(frac=frac, random_state=random_state)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = reduce_sample(df, 0.3, seed)\n",
    "# df_cleaned = reduce_sample(df_cleaned, 0.1, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6aa034",
   "metadata": {},
   "source": [
    "Columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "477335fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the original dataset:\n",
      "\n",
      "Index(['target', 'id', 'date', 'flag', 'user', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in the original dataset:\\n\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2a23b",
   "metadata": {},
   "source": [
    "Example of an Row in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "689c8807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1974058893</td>\n",
       "      <td>Sat May 30 12:21:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BrookeAmanda</td>\n",
       "      <td>Ok it's only been a couple hours since dad has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1998068077</td>\n",
       "      <td>Mon Jun 01 17:56:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KarinaKornacka</td>\n",
       "      <td>@graceofrhythm HAHA no i would never do that!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1999729993</td>\n",
       "      <td>Mon Jun 01 20:43:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>stevegaghagen</td>\n",
       "      <td>Law of Attraction Creations: Law of Attraction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2006627206</td>\n",
       "      <td>Tue Jun 02 11:26:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Hecie</td>\n",
       "      <td>is ordering ticketsssss  EEEE (: &amp;lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1991292674</td>\n",
       "      <td>Mon Jun 01 06:46:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>shanaloren</td>\n",
       "      <td>@STO_MAC nah im not mad at u....luv u too</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag            user  \\\n",
       "0       0  1974058893  Sat May 30 12:21:30 PDT 2009  NO_QUERY    BrookeAmanda   \n",
       "1       4  1998068077  Mon Jun 01 17:56:23 PDT 2009  NO_QUERY  KarinaKornacka   \n",
       "2       4  1999729993  Mon Jun 01 20:43:12 PDT 2009  NO_QUERY   stevegaghagen   \n",
       "3       4  2006627206  Tue Jun 02 11:26:46 PDT 2009  NO_QUERY           Hecie   \n",
       "4       4  1991292674  Mon Jun 01 06:46:16 PDT 2009  NO_QUERY      shanaloren   \n",
       "\n",
       "                                                text  \n",
       "0  Ok it's only been a couple hours since dad has...  \n",
       "1  @graceofrhythm HAHA no i would never do that!!...  \n",
       "2  Law of Attraction Creations: Law of Attraction...  \n",
       "3             is ordering ticketsssss  EEEE (: &lt;3  \n",
       "4         @STO_MAC nah im not mad at u....luv u too   "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d146b21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>480000.000000</td>\n",
       "      <td>4.800000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.997267</td>\n",
       "      <td>1.999221e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.936300e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467810e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.956973e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.002237e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.177253e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.329205e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              target            id\n",
       "count  480000.000000  4.800000e+05\n",
       "mean        1.997267  1.999221e+09\n",
       "std         2.000000  1.936300e+08\n",
       "min         0.000000  1.467810e+09\n",
       "25%         0.000000  1.956973e+09\n",
       "50%         0.000000  2.002237e+09\n",
       "75%         4.000000  2.177253e+09\n",
       "max         4.000000  2.329205e+09"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9c1c4",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "32c21871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df, drop_columns):\n",
    "    df_cleaned = df.dropna()\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    df_cleaned = df_cleaned.drop(columns=drop_columns)\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "    df_cleaned.describe()\n",
    "    return df_cleaned\n",
    "\n",
    "df_cleaned = clean_dataset(df, [\"date\", \"id\", \"flag\", \"user\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a6f5b54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok it's only been a couple hours since dad has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>@graceofrhythm HAHA no i would never do that!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Law of Attraction Creations: Law of Attraction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>is ordering ticketsssss  EEEE (: &amp;lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@STO_MAC nah im not mad at u....luv u too</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  Ok it's only been a couple hours since dad has...\n",
       "1       4  @graceofrhythm HAHA no i would never do that!!...\n",
       "2       4  Law of Attraction Creations: Law of Attraction...\n",
       "3       4             is ordering ticketsssss  EEEE (: &lt;3\n",
       "4       4         @STO_MAC nah im not mad at u....luv u too "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b04e21",
   "metadata": {},
   "source": [
    "Remove twitter tag and hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "53ca08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_hashtag(df_cleaned):\n",
    "    df_cleaned['text'] = df_cleaned['text'].apply(lambda x: re.sub(r\"http\\S+|@\\S+|#\\S+\", \"\", x))\n",
    "    return df_cleaned\n",
    "\n",
    "df_cleaned = remove_hashtag(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ba2ef7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok it's only been a couple hours since dad has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>HAHA no i would never do that!!! I actually m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Law of Attraction Creations: Law of Attraction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>is ordering ticketsssss  EEEE (: &amp;lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>nah im not mad at u....luv u too</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  Ok it's only been a couple hours since dad has...\n",
       "1       4   HAHA no i would never do that!!! I actually m...\n",
       "2       4  Law of Attraction Creations: Law of Attraction...\n",
       "3       4             is ordering ticketsssss  EEEE (: &lt;3\n",
       "4       4                  nah im not mad at u....luv u too "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268dec6",
   "metadata": {},
   "source": [
    "convert target back to -1 0 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e9ee2f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "-1    240328\n",
       " 1    239672\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_unitary_target(df_cleaned, target_column):\n",
    "    df_cleaned[target_column] = df_cleaned[target_column].map({0: -1, 2: 0, 4: 1})\n",
    "    return df_cleaned\n",
    "\n",
    "df_cleaned = convert_to_unitary_target(df_cleaned, 'target')\n",
    "df_cleaned['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d13f1",
   "metadata": {},
   "source": [
    "Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eb2af120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [ok, it, only, been, couple, hours, since, dad...\n",
       "1    [haha, no, would, never, do, that, actually, m...\n",
       "2    [law, of, attraction, creations, law, of, attr...\n",
       "3                [is, ordering, ticketsssss, eeee, lt]\n",
       "4                    [nah, im, not, mad, at, luv, too]\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(df_cleaned, text_column, tokenized_text_column):\n",
    "    from gensim.utils import simple_preprocess\n",
    "    # Tokenize the text column to get the new column 'tokenized_text'\n",
    "    df_cleaned[tokenized_text_column] = [simple_preprocess(line, deacc=True) for line in df_cleaned[text_column]]\n",
    "    return df_cleaned\n",
    "    \n",
    "df_cleaned = tokenize_text(df_cleaned, 'text', 'tokenized_text')\n",
    "df_cleaned['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c96433",
   "metadata": {},
   "source": [
    "# Stemming & Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6c904fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_be_stemmed = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d4957",
   "metadata": {},
   "source": [
    "### PoterStammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0ad6379b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [ok, it, onli, been, coupl, hour, sinc, dad, h...\n",
       "1    [haha, no, would, never, do, that, actual, mad...\n",
       "2    [law, of, attract, creation, law, of, attract,...\n",
       "3                   [is, order, ticketsssss, eeee, lt]\n",
       "4                    [nah, im, not, mad, at, luv, too]\n",
       "5    [centuri, room, tast, the, rainbow, with, your...\n",
       "6    [ari, also, got, servic, award, for, the, comm...\n",
       "7    [man, thought, somethin, wa, fina, go, done, s...\n",
       "8           [leav, moscow, when, it, final, get, warm]\n",
       "9                           [wish, got, summer, break]\n",
       "Name: stemmed_text, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def porter_stemmer_on_text(df_to_be_stemmed, token_text_column, stemmed_text_column):\n",
    "    from gensim.parsing.porter import PorterStemmer\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    df_potter_stemmed = df_to_be_stemmed.copy()\n",
    "    # Get the stemmed_tokens\n",
    "    df_potter_stemmed[stemmed_text_column] = [[porter_stemmer.stem(word) for word in tokens] \n",
    "                                        for tokens in df_potter_stemmed[token_text_column]]  \n",
    "    return df_potter_stemmed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fc115",
   "metadata": {},
   "source": [
    "### Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e0011d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lancaster_stemmer_on_text(df_to_be_stemmed, token_text_column, stemmed_text_column):\n",
    "    from nltk.stem.lancaster import LancasterStemmer\n",
    "    lancaster_stemmer = LancasterStemmer()\n",
    "    df_lancaster_stemmed = df_to_be_stemmed.copy()\n",
    "    # Get the stemmed_tokens\n",
    "    df_lancaster_stemmed[stemmed_text_column] = [[lancaster_stemmer.stem(word) for word in tokens] \n",
    "                                        for tokens in df_lancaster_stemmed[token_text_column]]\n",
    "    \n",
    "    return df_lancaster_stemmed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424002e9",
   "metadata": {},
   "source": [
    "### Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4020d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snowball_stemmer_on_text(df_to_be_stemmed, token_text_column, stemmed_text_column):\n",
    "    from nltk.stem.snowball import EnglishStemmer\n",
    "    snowball_stemmer = EnglishStemmer()\n",
    "    df_snowball_stemmed = df_to_be_stemmed.copy()\n",
    "    # Get the stemmed_tokens\n",
    "    df_snowball_stemmed[stemmed_text_column] = [[snowball_stemmer.stem(word) for word in tokens] \n",
    "                                        for tokens in df_snowball_stemmed[token_text_column]]\n",
    "    \n",
    "    return df_snowball_stemmed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453e96e",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a0c4ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(df_to_be_stemmed, token_text_column, lemmatized_text_column):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    df_lemmatized = df_to_be_stemmed.copy()\n",
    "    \n",
    "    # Get the lemmatized_tokens\n",
    "    df_lemmatized[lemmatized_text_column] = [[wordnet_lemmatizer.lemmatize(word) for word in tokens] \n",
    "                                          for tokens in df_lemmatized[token_text_column]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_potter_stemmed = porter_stemmer_on_text(df_to_be_stemmed, 'tokenized_text', 'stemmed_text')\n",
    "df_potter_stemmed['stemmed_text'].head(10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e0074",
   "metadata": {},
   "source": [
    "## Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643bda16",
   "metadata": {},
   "source": [
    "- Train data ( Subset of data for training ML Model) ~70%\n",
    "- Test data (Subset of data for testing ML Model trained from the train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2e99184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train sentiments\n",
      "target\n",
      "-1    168133\n",
      " 1    167867\n",
      "Name: count, dtype: int64\n",
      "Value counts for Test sentiments\n",
      "target\n",
      "-1    72195\n",
      " 1    71805\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "    index                                       stemmed_text\n",
      "0  266527  [lil, is, move, to, alabama, at, the, end, of,...\n",
      "1  401839  [hangov, and, unfortun, there, lot, of, work, ...\n",
      "2  218733                 [damn, won, thi, round, homi, lol]\n",
      "3  435715       [dont, you, worri, hear, the, tequila, call]\n",
      "4  203570  [wish, could, join, you, on, fridai, but, on, ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_test(data, sentiment_value_col, tokenised_text_col, test_size=0.3, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split( data[tokenised_text_col],\n",
    "                                                        data[sentiment_value_col], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(df_potter_stemmed, 'target', 'stemmed_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c2ac1",
   "metadata": {},
   "source": [
    "# Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339bfb7",
   "metadata": {},
   "source": [
    "## Save-gram approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87617e6",
   "metadata": {},
   "source": [
    "### Generate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "16df688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word2vec_model(stemmed_df, filename, stem_column_name):\n",
    "    from gensim.models import Word2Vec\n",
    "    # Skip-gram model (sg = 1)\n",
    "    vector_size = 1000\n",
    "    window = 3\n",
    "    min_count = 1\n",
    "    workers = 3\n",
    "    sg = 1\n",
    "    filename = filename + \".model\"\n",
    "    stemmed_tokens = pd.Series(stemmed_df[stem_column_name]).values\n",
    "    # Train the Word2Vec Model\n",
    "    w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, vector_size = vector_size, workers = workers, window = window, sg = sg)\n",
    "    w2v_model.save(filename)\n",
    "    \n",
    "    return w2v_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d34b8",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7d749099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'action':\n",
      "1725\n",
      "212909\n",
      "Length of the vector generated for a word\n",
      "1000\n",
      "Print the length after taking average of all word vectors in a sentence:\n",
      "[ 1.10997356e-01  1.29606901e-02  1.31717682e-01  1.03594407e-01\n",
      " -1.26589434e-02 -5.93637340e-02  4.32857983e-02  7.32175168e-03\n",
      " -1.02565773e-01  1.25884488e-01  1.30147506e-02 -4.81054820e-02\n",
      "  3.28933215e-03  3.36098783e-02  1.23340942e-01 -2.79482193e-02\n",
      " -1.01861104e-01  1.62472036e-02  1.87799558e-02 -2.11371675e-01\n",
      "  8.02890286e-02 -2.31562387e-02  4.07561325e-02  1.74559071e-03\n",
      "  7.90047348e-02 -3.33950645e-03  9.08088088e-02 -3.37177813e-02\n",
      " -2.34114110e-01  7.87946358e-02  7.36722648e-02 -7.28675872e-02\n",
      " -3.23941149e-02 -1.14671990e-01  1.14956014e-01  2.42509134e-02\n",
      "  5.61343543e-02 -3.91341671e-02 -1.00916252e-01 -1.79448381e-01\n",
      " -1.53343290e-01  7.30233118e-02 -6.34375215e-02  1.03500888e-01\n",
      " -8.34881514e-02 -3.69986929e-02 -1.38792112e-01  1.04046136e-01\n",
      " -1.43292069e-01  2.93894075e-02 -4.37203376e-03 -3.57635505e-02\n",
      " -2.25981744e-03 -8.05871934e-02 -8.97545833e-03 -4.93321456e-02\n",
      "  1.19227961e-01 -4.51083854e-02 -1.06408246e-01  2.82854084e-02\n",
      " -5.35619222e-02  1.11117540e-02 -2.91707031e-02 -3.96702439e-02\n",
      " -3.48017924e-02 -1.85176749e-02 -9.73499566e-02  2.00531572e-01\n",
      "  5.52434847e-03  2.83581503e-02  1.06111109e-01 -7.10869059e-02\n",
      "  2.49486323e-02 -1.40978619e-01 -1.61489733e-02  5.91504872e-02\n",
      "  4.11929935e-02 -3.88183706e-02  6.02915809e-02 -8.90987180e-03\n",
      " -2.89313681e-02  4.33984511e-02 -5.46783991e-02  1.79156035e-01\n",
      " -2.04483464e-01  3.85410115e-02 -7.86583684e-03  1.07670464e-01\n",
      "  7.14292377e-02  7.31912032e-02  6.13483973e-02 -3.03262584e-02\n",
      " -3.10864858e-02  1.12710185e-01  2.17601791e-01  7.40144327e-02\n",
      "  1.44863307e-01 -1.50425985e-01 -7.42057990e-03  3.16315480e-02\n",
      " -1.00124434e-01  1.54270247e-01  2.35866848e-02  7.22317323e-02\n",
      "  1.92705467e-02  3.57748126e-03  4.03131079e-03  4.27066199e-02\n",
      "  1.72310825e-02 -7.45052239e-03 -1.19211366e-02 -6.48726709e-03\n",
      "  8.75086561e-02  6.23857453e-02  7.47908503e-02 -5.57512930e-03\n",
      "  5.52260354e-02 -1.36166170e-01 -4.22277004e-02 -1.16695873e-01\n",
      "  4.83043585e-03  1.00017391e-01  9.92755815e-02  2.60176547e-02\n",
      " -7.77866393e-02  8.99192765e-02 -1.07689612e-01  8.32992233e-03\n",
      " -1.53769711e-02  6.68373704e-02  1.13422975e-01  7.71947354e-02\n",
      " -8.79876316e-02 -1.37177873e-02 -3.96134034e-02  3.07523068e-02\n",
      " -7.35463426e-02 -2.81935520e-02 -9.86082256e-02 -8.38218182e-02\n",
      "  4.15385067e-02 -9.86675918e-02 -4.26273979e-02 -3.46166454e-02\n",
      " -6.90579787e-02 -2.79214904e-02  1.11134768e-01  6.57734647e-02\n",
      " -3.36933956e-02  3.66339013e-02  1.02666005e-01 -1.52487129e-01\n",
      " -4.32480760e-02  2.04780519e-01 -2.47978047e-01  7.37496316e-02\n",
      "  1.37786135e-01 -4.51922566e-02 -7.90359154e-02  1.52046472e-01\n",
      "  3.51412818e-02  9.16820988e-02 -1.35077029e-01 -2.27823690e-01\n",
      " -1.22149713e-01  2.09869474e-01  1.25413090e-01  4.38225381e-02\n",
      " -6.10699952e-02  1.57533631e-01 -5.01932018e-02  1.31327078e-01\n",
      "  8.43236630e-04 -3.99751998e-02 -1.06565550e-01  4.93947230e-02\n",
      " -4.53485660e-02 -1.52678285e-02  1.24329431e-02  1.01395816e-01\n",
      "  1.03759974e-01  1.07340947e-01  5.47220074e-02  3.19131352e-02\n",
      " -3.38805132e-02 -4.29776199e-02 -5.82951307e-02 -4.47253622e-02\n",
      " -8.24074447e-02  9.02228337e-03  1.57776773e-01 -9.50039178e-02\n",
      "  7.52924830e-02 -7.04100356e-02  1.15821570e-01 -8.79554003e-02\n",
      " -1.56737790e-02 -7.78567716e-02  1.07534811e-01 -5.89575209e-02\n",
      " -1.16702877e-01 -1.29234919e-03  1.94411024e-01 -8.02063197e-03\n",
      "  1.81666650e-02 -8.25809911e-02 -1.67936966e-01 -8.25755745e-02\n",
      " -2.34802254e-02  3.09521407e-02  5.95545992e-02 -5.00493683e-03\n",
      " -9.78347138e-02  7.19991848e-02  7.85283837e-03 -1.83910161e-01\n",
      " -2.78629865e-02  2.02421471e-02 -1.21255718e-01 -5.68937548e-02\n",
      "  1.13949236e-02  1.17831051e-01  1.33957580e-01  8.93169735e-03\n",
      " -1.09452456e-01 -8.28401819e-02  1.20509677e-01 -6.45055994e-02\n",
      "  1.31028667e-01  5.11647062e-03  2.49801874e-02 -6.58849925e-02\n",
      "  9.21916366e-02  1.66137844e-01 -1.10209942e-01 -1.17691350e-03\n",
      "  4.59596589e-02 -1.48177862e-01  7.95035958e-02 -7.17677400e-02\n",
      " -5.63244708e-03  9.38996579e-03 -5.72655089e-02  1.11549027e-01\n",
      "  5.50101623e-02  3.85979712e-02  5.78182042e-02 -4.34007077e-03\n",
      "  1.51124336e-02  5.76845407e-02  9.96113792e-02 -2.05084682e-02\n",
      " -1.21402241e-01 -7.03097507e-02 -1.65073313e-02  1.32228136e-02\n",
      "  1.33505195e-01  1.55406883e-02 -4.25930806e-02  6.07380196e-02\n",
      " -1.44973546e-01  1.47489697e-01 -8.86318237e-02 -3.20370048e-02\n",
      "  4.19960031e-03 -5.08630322e-03 -4.56457809e-02  2.48142824e-01\n",
      " -1.74972370e-01  1.05645515e-01 -8.12137946e-02  3.47591266e-02\n",
      " -3.11101135e-03  6.91191778e-02 -2.97557525e-02  1.12800911e-01\n",
      "  4.28934805e-02 -1.01509675e-01 -1.02771513e-01  1.39531329e-01\n",
      "  3.40801068e-02  3.59231085e-02 -1.45290166e-01  2.04003532e-03\n",
      "  2.03770593e-01  1.89241581e-02  4.65682037e-02 -5.34334593e-02\n",
      " -5.56434914e-02  7.24193305e-02 -1.06636308e-01  1.31402299e-01\n",
      "  1.44416049e-01  4.63808514e-02  7.83480182e-02 -1.16841339e-01\n",
      "  2.36526988e-02  2.65423786e-02  3.28642353e-02  4.92889583e-02\n",
      " -1.80054352e-01  1.75375268e-01 -6.48533404e-02 -1.04867354e-01\n",
      " -1.69796944e-02 -1.08357191e-01 -1.12987671e-03  1.01200864e-01\n",
      "  3.58637944e-02 -1.44540682e-01 -7.87585974e-03 -9.58065018e-02\n",
      "  7.24522099e-02  5.73994443e-02  4.33201864e-02  4.15418632e-02\n",
      " -4.16689459e-03 -1.33505702e-01  5.66928685e-02 -1.87942594e-01\n",
      "  6.58365563e-02  1.86901204e-02  6.56684712e-02 -5.44764772e-02\n",
      " -7.62342513e-02 -3.62415910e-02 -2.99837478e-02 -1.07288092e-01\n",
      "  2.88253780e-02 -1.51442690e-02  4.01080474e-02  6.04948215e-02\n",
      "  5.54578006e-02 -2.53953356e-02 -2.67346315e-02 -1.39635593e-01\n",
      " -2.56532189e-02  3.55839431e-02 -1.25169545e-01 -1.15879871e-01\n",
      "  1.77337322e-02  9.92678776e-02 -4.55031358e-03 -1.42615020e-01\n",
      " -5.53598888e-02 -1.82559546e-02 -2.24058628e-01 -4.48966846e-02\n",
      " -2.86894254e-02  3.02160699e-02 -1.78840999e-02 -9.16749518e-03\n",
      " -7.42747858e-02  1.88934430e-01 -9.18457583e-02  2.53158808e-01\n",
      " -2.88778078e-02 -3.73136364e-02  7.02712610e-02 -4.96727191e-02\n",
      " -1.13833569e-01  2.94873063e-02 -4.65275608e-02 -5.39967511e-03\n",
      " -1.27030732e-02 -6.64404919e-03  2.43582148e-02  2.55438555e-02\n",
      " -1.52943745e-01 -8.85215327e-02 -7.19524175e-02 -1.41672373e-01\n",
      " -5.42700067e-02  2.92047542e-02 -9.77756009e-02  7.20150918e-02\n",
      "  6.59130216e-02 -5.61261140e-02  5.24458326e-02  6.64536357e-02\n",
      "  4.21753852e-03  9.34613794e-02  1.24534875e-01  8.28280747e-02\n",
      "  6.58247387e-03  3.88327613e-02  2.19152682e-02  9.90374088e-02\n",
      "  1.11112170e-01  3.01346537e-02  4.43251953e-02 -1.31864741e-01\n",
      " -1.13722861e-01  1.94982197e-02  1.19378455e-01  1.76377874e-02\n",
      "  5.73721491e-02  5.62975369e-02 -7.88488612e-02  4.99590747e-02\n",
      "  3.82372886e-02  1.06580555e-01 -8.85803699e-02 -8.32581744e-02\n",
      "  7.76914358e-02 -8.36793780e-02  2.48022862e-02  9.39297155e-02\n",
      " -3.24205086e-02  3.98433656e-02 -1.07731506e-01 -9.01383758e-02\n",
      " -1.23193868e-01 -2.63784193e-02 -6.32601697e-03 -8.46878588e-02\n",
      "  3.04920021e-02  6.08231388e-02  1.45750284e-01 -3.48740108e-02\n",
      " -6.78205490e-02 -1.92308575e-01 -6.07508700e-03  7.72104785e-02\n",
      " -1.52236465e-02 -4.39113602e-02 -6.29529655e-02  9.61862355e-02\n",
      " -1.61760733e-01 -4.18518074e-02 -8.49170908e-02 -6.06929883e-02\n",
      " -8.54707658e-02  3.06317192e-02  8.92560780e-02  7.73920044e-02\n",
      "  7.44500430e-03  3.08115035e-02  2.33847629e-02  8.06326494e-02\n",
      "  6.36758357e-02 -1.52246710e-02 -4.58313385e-03  5.03986813e-02\n",
      "  2.75323510e-01 -6.03521019e-02  9.21574682e-02 -5.95536083e-02\n",
      "  5.15604503e-02  2.02989914e-02  8.48562270e-02  5.38661852e-02\n",
      " -1.26224756e-01 -2.19045743e-01 -8.63341689e-02 -4.35738266e-03\n",
      " -9.35983378e-04 -2.23699674e-01  8.83150399e-02 -4.85316617e-03\n",
      "  9.11983624e-02  9.60570946e-02 -1.88335869e-02  3.93122844e-02\n",
      "  1.83517840e-02 -2.83665843e-02 -5.77431433e-02 -1.46788314e-01\n",
      "  8.37008120e-04  4.16639671e-02 -8.78576934e-02 -3.81453894e-02\n",
      "  4.10002917e-02 -3.99717502e-02  8.40949565e-02  9.46383998e-02\n",
      "  8.36792514e-02 -3.54632139e-02 -1.65029541e-01 -1.05414607e-01\n",
      " -1.03399917e-01 -2.11196139e-01 -1.68144912e-03  1.24645501e-01\n",
      " -1.10521428e-01  7.95287173e-03  1.22983076e-01  1.92843556e-01\n",
      "  2.85278708e-02 -6.66173100e-02 -3.97163741e-02 -3.99719179e-02\n",
      "  2.58427057e-02  3.34336497e-02 -9.89673659e-02 -2.04010541e-03\n",
      " -1.71394069e-02 -1.11028835e-01  2.61435052e-04  1.57299114e-03\n",
      " -3.80826294e-02  1.25136331e-01  8.11995193e-02  6.82450458e-02\n",
      " -9.68381017e-02 -1.53856069e-01  1.00668795e-01  2.26363093e-01\n",
      "  9.47405174e-02 -5.69838919e-02 -7.82294571e-02  8.60958025e-02\n",
      "  4.84314188e-03  1.10965371e-02 -3.14273462e-02 -2.58088917e-01\n",
      " -1.42291635e-01 -1.32640377e-01 -1.09241143e-01  1.75876766e-01\n",
      " -2.87420675e-02 -8.14283174e-03  1.19263463e-01  8.71829018e-02\n",
      "  1.24272509e-02  7.72695318e-02 -2.94151548e-02  6.19667768e-02\n",
      " -4.67806756e-02 -2.08576679e-01  1.95669662e-02  1.28188938e-01\n",
      "  7.28144869e-02  4.35030945e-02  2.78023742e-02  1.62762016e-01\n",
      " -9.06793252e-02 -1.17566302e-01  6.46231771e-02 -1.84619687e-02\n",
      "  5.04678786e-02 -8.45049545e-02 -4.97737229e-02 -9.49436054e-02\n",
      " -1.07060008e-01 -1.69228807e-01  3.74852843e-03 -9.11137238e-02\n",
      " -4.15270403e-03 -3.57095189e-02 -2.92847864e-03 -1.54154077e-02\n",
      " -3.07868491e-03 -7.11883903e-02  2.67344508e-02 -6.16828687e-02\n",
      "  1.06929936e-01  1.25831962e-01  8.40481892e-02  3.72228213e-02\n",
      "  2.23501563e-01 -1.24051413e-02  1.04322918e-01  5.45594543e-02\n",
      " -6.93522468e-02  1.79145187e-02 -3.68451774e-02 -9.81475264e-02\n",
      "  5.41389622e-02  2.50589829e-02 -1.15436286e-01  4.16849479e-02\n",
      " -3.37910168e-02 -2.09698137e-02  8.73247758e-02  1.59320101e-01\n",
      " -1.40507475e-01  3.91022041e-02  8.56641233e-02  1.21474214e-01\n",
      " -6.92359917e-03 -8.51002708e-02  3.50693590e-03  1.48644047e-02\n",
      " -2.46693417e-02  5.71675599e-02  4.33722995e-02  8.14913809e-02\n",
      "  9.47463233e-03  1.25469133e-01  9.22875013e-03 -8.82349983e-02\n",
      " -1.21316202e-01  8.69696029e-03 -9.66623425e-04  8.37826282e-02\n",
      " -3.68863121e-02  1.36655033e-01 -4.28366959e-02  2.29207333e-02\n",
      " -5.58233894e-02 -5.43634854e-02 -1.09582290e-01  2.13269945e-02\n",
      "  1.13018729e-01 -1.26850173e-01 -1.00813724e-01  1.54098734e-01\n",
      " -1.26525521e-01  8.32051411e-02 -4.42058071e-02 -7.72684142e-02\n",
      " -4.97284196e-02  7.45963678e-03 -1.27639785e-01  3.07688303e-02\n",
      " -1.88268237e-02 -5.71015812e-02 -7.70513900e-03  2.88751591e-02\n",
      " -7.55609618e-03 -6.82734549e-02 -5.48493350e-03 -1.21778019e-01\n",
      "  2.50091292e-02  7.50749977e-03 -1.07478350e-01 -4.42019962e-02\n",
      " -2.47691330e-02 -1.82501525e-02  7.59003311e-02  9.66505930e-02\n",
      " -8.45630746e-03 -1.04240842e-01 -6.18342012e-02  6.90004602e-02\n",
      "  3.12378984e-02 -6.81928247e-02  1.76547579e-02 -1.11522771e-01\n",
      "  2.24783756e-02 -4.20750082e-02  9.38742757e-02 -4.62499335e-02\n",
      "  1.04450621e-01  1.09622478e-01 -1.97910313e-02 -1.69839218e-01\n",
      "  4.40942310e-02  2.71838550e-02 -1.52961444e-02  6.19972683e-02\n",
      "  6.97072521e-02  8.61703753e-02  4.95492332e-02  2.84831524e-02\n",
      " -1.24967229e-02 -9.31970626e-02 -2.61491537e-02 -3.51219550e-02\n",
      "  7.12800249e-02 -1.03553437e-01  4.25141044e-02 -1.63595695e-02\n",
      " -4.07363335e-03  3.22242267e-02  3.06741912e-02  6.55938983e-02\n",
      "  6.18579648e-02 -4.84398715e-02  1.29736438e-02  7.05589354e-02\n",
      " -5.23997322e-02  1.76614344e-01 -4.87958826e-03 -1.09667465e-01\n",
      "  7.82312378e-02  2.63483040e-02 -6.56596199e-02 -6.46419637e-03\n",
      " -6.18354566e-02 -1.09585153e-03 -2.18775738e-02  8.66352767e-03\n",
      " -9.80892871e-03 -1.74956575e-01 -1.67178765e-01  4.11689989e-02\n",
      " -1.21631578e-01 -3.52366449e-04  2.91427784e-03 -6.15699776e-02\n",
      "  8.49137232e-02  1.94921456e-02  9.30591859e-03  7.69679844e-02\n",
      "  1.90125424e-02 -8.81842524e-03  9.38189030e-02  5.92539832e-02\n",
      "  4.92657023e-03 -6.04864992e-02 -2.77646799e-02 -6.74256831e-02\n",
      " -1.33865057e-02  8.16080868e-02  1.14587441e-01 -7.99540337e-03\n",
      " -1.97788194e-01 -3.46579514e-02 -2.85060350e-02 -1.16292231e-01\n",
      " -8.27630907e-02 -4.81241681e-02 -7.06227720e-02  4.43991162e-02\n",
      "  2.37167124e-02 -1.51774529e-02  1.05965167e-01 -1.03298396e-01\n",
      "  1.19158648e-01  1.74554139e-01  2.76780706e-02  3.55087742e-02\n",
      "  7.94930309e-02 -1.73595338e-03  4.06012498e-02 -6.73749670e-02\n",
      " -2.59981200e-04  2.41277665e-02 -1.50284404e-02 -7.26440363e-03\n",
      "  2.96052825e-02 -1.14877522e-01  5.15115038e-02  5.80897331e-02\n",
      " -6.80356622e-02 -1.24548033e-01  1.48039907e-01 -1.69517130e-01\n",
      "  2.21163910e-02 -6.88730031e-02 -1.08256653e-01  7.05252811e-02\n",
      " -1.37395978e-01 -1.49017915e-01 -7.48077556e-02  6.37813658e-02\n",
      "  1.08286567e-01  2.79918667e-02  9.00670327e-03 -1.44214883e-01\n",
      "  1.16714604e-01 -6.54922649e-02 -9.63308513e-02  3.25732082e-02\n",
      "  3.12127341e-02  1.91251010e-01 -9.68952179e-02  3.56795043e-02\n",
      " -7.32958987e-02 -2.22402159e-02 -1.30569041e-01  6.03490584e-02\n",
      "  4.83475775e-02 -2.28008274e-02 -1.35871053e-01  6.02727458e-02\n",
      "  4.12628725e-02 -4.70951907e-02 -3.40218544e-02 -7.37160258e-03\n",
      "  1.14864729e-01 -9.40913856e-02 -2.57449560e-02 -9.28562731e-02\n",
      " -2.38446109e-02 -8.57313499e-02 -3.27287950e-02 -4.11883369e-02\n",
      " -2.92170830e-02  3.67466710e-03 -5.78422984e-03 -1.64440110e-01\n",
      "  1.34960681e-01 -1.04116179e-01 -8.43907744e-02  7.34257177e-02\n",
      "  5.38918152e-02 -9.91807207e-02 -2.39287894e-02  1.19927876e-01\n",
      " -2.52463706e-02 -1.70255855e-01  5.05672023e-02  3.22680436e-02\n",
      "  1.31816342e-01 -1.59642786e-01  3.06483693e-02 -9.77207050e-02\n",
      " -1.40906796e-01 -4.46374826e-02  7.35954121e-02  3.08524705e-02\n",
      "  1.46700978e-01  1.63874179e-02 -4.29766737e-02  3.45158391e-02\n",
      " -2.10785661e-02  9.28352103e-02  8.42807516e-02  3.84681411e-02\n",
      "  4.15274315e-02  1.01433724e-01  6.49714619e-02  1.18299089e-02\n",
      " -9.84088257e-02 -7.92039335e-02  2.04633246e-03  5.38162701e-03\n",
      "  9.55752507e-02 -2.25083716e-02  2.33773943e-02 -1.45328090e-01\n",
      "  1.41229657e-02 -4.06620912e-02 -2.51206029e-02 -1.87833868e-02\n",
      "  3.40287574e-03  9.69502982e-03  1.34559721e-01 -4.98222262e-02\n",
      "  8.01753551e-02  5.94873205e-02 -4.94954959e-02 -1.73984934e-02\n",
      " -2.42219493e-02  1.01065017e-01  1.29444245e-02 -9.04860049e-02\n",
      " -1.73472211e-01 -1.55358836e-02  1.86750129e-01  2.24030223e-02\n",
      "  3.90569381e-02 -8.50417838e-02 -9.22465026e-02  1.84830576e-02\n",
      " -3.57978977e-02  1.37651488e-01  1.97802596e-02  3.80897373e-02\n",
      " -4.66432571e-02  2.46936660e-02  1.08522400e-02 -7.12542310e-02\n",
      " -9.66765136e-02  3.39401253e-02 -4.69913855e-02 -4.52530012e-02\n",
      "  8.63996819e-02  9.66318697e-02  7.42546916e-02  3.70020159e-02\n",
      " -3.86266932e-02 -2.38162335e-02  3.23878601e-02  7.58730620e-02\n",
      " -8.20186585e-02  1.12884305e-01  6.15419932e-02 -1.60547309e-02\n",
      "  7.86553994e-02 -7.87342489e-02 -1.41770374e-02 -8.79726186e-02\n",
      " -5.04005253e-02 -3.62107232e-02 -1.21975899e-01 -4.70113456e-02\n",
      "  9.04343352e-02 -7.22783208e-02  5.07672131e-02  4.58058827e-02\n",
      "  1.74140036e-02 -3.95548753e-02 -3.26870126e-03  2.05761585e-02\n",
      "  2.80177104e-03 -4.24311794e-02  2.65003368e-02  3.53997275e-02\n",
      " -3.41336168e-02  1.07898921e-01  2.06278022e-02  1.64214328e-01\n",
      " -1.05140761e-01 -1.09560892e-01  5.97612560e-02  7.21692964e-02\n",
      " -9.20623392e-02 -2.87299827e-02 -1.71987966e-01 -1.18769966e-01\n",
      " -7.26544708e-02 -7.19775707e-02 -3.54033709e-02  3.74545865e-02\n",
      "  2.80442256e-02 -2.99936589e-02 -2.65559219e-02 -6.07036315e-02\n",
      " -1.54780537e-01 -2.85639409e-02  2.35715751e-02  7.41562769e-02\n",
      "  1.32140055e-01 -1.94398075e-01 -1.54692326e-02  2.78465506e-02\n",
      " -7.71434382e-02  2.07507759e-01 -5.36345318e-03 -3.40653807e-02\n",
      " -2.55367551e-02  1.51121289e-01 -8.06485116e-02 -6.00160211e-02\n",
      "  8.85218307e-02 -1.14176981e-01  9.45579261e-03 -1.31196557e-02\n",
      " -2.00658459e-02  8.21022242e-02 -1.75535306e-01  6.46656528e-02\n",
      " -6.60765693e-02 -1.28939062e-01 -1.45618647e-01 -2.18759757e-02\n",
      "  6.09483719e-02 -2.15804819e-02  1.06504492e-01 -5.65699190e-02\n",
      " -6.91185221e-02 -1.98145472e-02  4.83580977e-02  1.67455316e-01\n",
      " -1.78874001e-01 -6.72326656e-03 -5.60153369e-03  8.22198465e-02\n",
      " -4.07511741e-02 -6.36621118e-02 -4.05150689e-02  7.99205303e-02\n",
      " -2.86550764e-02 -4.53630276e-02 -8.80287290e-02 -8.67403299e-02\n",
      "  5.24570793e-02  6.13436475e-02  4.29085270e-02 -8.15680027e-02\n",
      " -1.11968659e-01  3.33409593e-03 -2.32663173e-02 -1.77670687e-01\n",
      "  1.75042003e-02 -4.36693020e-02 -1.40654836e-02  5.96985407e-02\n",
      " -1.78999640e-02 -1.01662882e-01  1.60111353e-01  9.88513138e-03\n",
      "  7.65367523e-02  1.46940991e-01  1.22548910e-02 -1.20858915e-01\n",
      "  1.16008379e-01 -5.68708479e-02 -6.04711398e-02  1.13948338e-01\n",
      "  1.57704696e-01  9.26900953e-02 -5.75814396e-02  5.26734330e-02\n",
      "  7.22621456e-02  1.09215096e-01  2.16020979e-02 -9.36270803e-02\n",
      "  7.97582790e-02 -6.95524439e-02  1.14726722e-01 -2.78773159e-02\n",
      " -3.45337503e-02 -1.30947024e-01 -5.31449467e-02 -1.25354126e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "# Load the model from the model file\n",
    "vector_size = 1000\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "\n",
    "def load_word2vec_model(reduced_mode_file, stem_column_name):\n",
    "    sg_w2v_model = Word2Vec.load(reduced_mode_file)\n",
    "    sg_w2v_model_wv = sg_w2v_model.wv\n",
    "    # Unique ID of the word\n",
    "    print(\"Index of the word 'action':\")\n",
    "    print(sg_w2v_model_wv.key_to_index[\"action\"])\n",
    "    # Total number of the words \n",
    "    print(len(sg_w2v_model_wv.key_to_index))\n",
    "    # Print the size of the word2vec vector for one word\n",
    "    print(\"Length of the vector generated for a word\")\n",
    "    print(len(sg_w2v_model_wv['action']))\n",
    "    # Get the mean for the vectors for an example review\n",
    "    print(\"Print the length after taking average of all word vectors in a sentence:\")\n",
    "    print(np.mean([sg_w2v_model_wv[token] for token in df_potter_stemmed[stem_column_name][0]], axis=0))\n",
    "    return sg_w2v_model\n",
    "    \n",
    "sg_w2v_model = load_word2vec_model(\"../model/word2vec_1000savegram.model\", 'stemmed_text')\n",
    "sg_w2v_model_wv = sg_w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9be7ef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "def save_word2vec_to_csv(X_set, sg_w2v_model_wv, stem_col, word2vec_filename):\n",
    "    with open(word2vec_filename, 'w+') as word2vec_file:\n",
    "        for index, row in X_set.iterrows():\n",
    "            model_vector = (np.mean([sg_w2v_model_wv[token] for token in row[stem_col]], axis=0))\n",
    "            v_norm = model_vector / (np.linalg.norm(model_vector) + 1e-16)\n",
    "            v_norm = v_norm.tolist()\n",
    "            if index == 0:\n",
    "                header = \",\".join(str(ele) for ele in range(1000))\n",
    "                word2vec_file.write(header)\n",
    "                word2vec_file.write(\"\\n\")\n",
    "            # Check if the line exists else it is vector of zeros\n",
    "            if type(v_norm) is list:  \n",
    "                line1 = \",\".join( [str(vector_element) for vector_element in v_norm] )\n",
    "            else:\n",
    "                line1 = \",\".join([str(0) for i in range(1000)])\n",
    "            word2vec_file.write(line1)\n",
    "            word2vec_file.write('\\n')\n",
    "           \n",
    "def save_word2vec_to_csv_round(X_set, sg_w2v_model_wv, stem_col, word2vec_filename):\n",
    "    with open(word2vec_filename, 'w+') as word2vec_file:\n",
    "        for index, row in X_set.iterrows():\n",
    "            model_vector = (np.mean([sg_w2v_model_wv[token] for token in row[stem_col]], axis=0))\n",
    "            v_norm = model_vector / (np.linalg.norm(model_vector) + 1e-16)\n",
    "            v_norm = np.round(v_norm, decimals = 6 ).tolist()\n",
    "            if index == 0:\n",
    "                header = \",\".join(str(ele) for ele in range(1000))\n",
    "                word2vec_file.write(header)\n",
    "                word2vec_file.write(\"\\n\")\n",
    "            # Check if the line exists else it is vector of zeros\n",
    "            if type(v_norm) is list:  \n",
    "                line1 = \",\".join( [str(vector_element) for vector_element in v_norm] )\n",
    "            else:\n",
    "                line1 = \",\".join([str(0) for i in range(1000)])\n",
    "            word2vec_file.write(line1)\n",
    "            word2vec_file.write('\\n') \n",
    "    \n",
    "train_X_word2vec_filename = \"../model/train_X_0.3_wv.csv\"\n",
    "test_X_word2vec_filename = \"../model/test_X_0.3_wv.csv\"\n",
    "\n",
    "train_X_word2vec_rounded_filename = \"../model/train_X_full_wv.csv\"\n",
    "test_X_word2vec_rounded_filename = \"../model/test_X_full_wv.csv\"\n",
    "\n",
    "save_word2vec_to_csv(X_train, sg_w2v_model_wv, \"stemmed_text\", train_X_word2vec_filename)\n",
    "save_word2vec_to_csv(X_test, sg_w2v_model_wv, \"stemmed_text\", test_X_word2vec_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce572e8",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0744f2",
   "metadata": {},
   "source": [
    "## Load Training and Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c0eb14b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.048102</td>\n",
       "      <td>0.015179</td>\n",
       "      <td>0.042033</td>\n",
       "      <td>0.032470</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.003463</td>\n",
       "      <td>0.040373</td>\n",
       "      <td>0.010535</td>\n",
       "      <td>-0.047874</td>\n",
       "      <td>0.038799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>-0.026251</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>-0.023194</td>\n",
       "      <td>0.032529</td>\n",
       "      <td>-0.014211</td>\n",
       "      <td>-0.028527</td>\n",
       "      <td>-0.027768</td>\n",
       "      <td>-0.004475</td>\n",
       "      <td>-0.044720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044427</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.045554</td>\n",
       "      <td>0.037365</td>\n",
       "      <td>-0.001599</td>\n",
       "      <td>-0.007755</td>\n",
       "      <td>0.014855</td>\n",
       "      <td>0.012154</td>\n",
       "      <td>-0.053852</td>\n",
       "      <td>0.039776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006429</td>\n",
       "      <td>-0.029311</td>\n",
       "      <td>0.030579</td>\n",
       "      <td>-0.026832</td>\n",
       "      <td>0.031129</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>-0.034800</td>\n",
       "      <td>-0.008332</td>\n",
       "      <td>-0.042234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038869</td>\n",
       "      <td>0.012879</td>\n",
       "      <td>0.054727</td>\n",
       "      <td>0.045526</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>-0.022000</td>\n",
       "      <td>0.039736</td>\n",
       "      <td>0.025695</td>\n",
       "      <td>-0.075243</td>\n",
       "      <td>0.055876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027775</td>\n",
       "      <td>-0.019863</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>0.037935</td>\n",
       "      <td>-0.007918</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>-0.032762</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>-0.036511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049190</td>\n",
       "      <td>0.018796</td>\n",
       "      <td>0.036048</td>\n",
       "      <td>0.039769</td>\n",
       "      <td>0.018974</td>\n",
       "      <td>-0.033573</td>\n",
       "      <td>0.011563</td>\n",
       "      <td>0.004603</td>\n",
       "      <td>-0.053011</td>\n",
       "      <td>0.063128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>-0.015801</td>\n",
       "      <td>0.025616</td>\n",
       "      <td>-0.017155</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>-0.015505</td>\n",
       "      <td>-0.021535</td>\n",
       "      <td>-0.054803</td>\n",
       "      <td>-0.007507</td>\n",
       "      <td>-0.024850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034049</td>\n",
       "      <td>0.006086</td>\n",
       "      <td>0.031242</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>-0.010132</td>\n",
       "      <td>-0.010940</td>\n",
       "      <td>0.038614</td>\n",
       "      <td>0.013778</td>\n",
       "      <td>-0.037274</td>\n",
       "      <td>0.035137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003304</td>\n",
       "      <td>-0.030206</td>\n",
       "      <td>0.025648</td>\n",
       "      <td>-0.028249</td>\n",
       "      <td>0.020745</td>\n",
       "      <td>-0.020653</td>\n",
       "      <td>-0.022377</td>\n",
       "      <td>-0.032110</td>\n",
       "      <td>-0.002292</td>\n",
       "      <td>-0.054542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.048102  0.015179  0.042033  0.032470 -0.000219 -0.003463  0.040373   \n",
       "1  0.044427  0.015315  0.045554  0.037365 -0.001599 -0.007755  0.014855   \n",
       "2  0.038869  0.012879  0.054727  0.045526  0.000535 -0.022000  0.039736   \n",
       "3  0.049190  0.018796  0.036048  0.039769  0.018974 -0.033573  0.011563   \n",
       "4  0.034049  0.006086  0.031242  0.038000 -0.010132 -0.010940  0.038614   \n",
       "\n",
       "          7         8         9  ...       990       991       992       993  \\\n",
       "0  0.010535 -0.047874  0.038799  ...  0.003958 -0.026251  0.041925 -0.023194   \n",
       "1  0.012154 -0.053852  0.039776  ... -0.006429 -0.029311  0.030579 -0.026832   \n",
       "2  0.025695 -0.075243  0.055876  ...  0.027775 -0.019863  0.000699 -0.000591   \n",
       "3  0.004603 -0.053011  0.063128  ...  0.000155 -0.015801  0.025616 -0.017155   \n",
       "4  0.013778 -0.037274  0.035137  ... -0.003304 -0.030206  0.025648 -0.028249   \n",
       "\n",
       "        994       995       996       997       998       999  \n",
       "0  0.032529 -0.014211 -0.028527 -0.027768 -0.004475 -0.044720  \n",
       "1  0.031129  0.007292 -0.018692 -0.034800 -0.008332 -0.042234  \n",
       "2  0.037935 -0.007918  0.000477 -0.032762  0.005987 -0.036511  \n",
       "3  0.030357 -0.015505 -0.021535 -0.054803 -0.007507 -0.024850  \n",
       "4  0.020745 -0.020653 -0.022377 -0.032110 -0.002292 -0.054542  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_train_w2v_from_csv(word2vec_filename):\n",
    "    train_word2vec_df = pd.read_csv(word2vec_filename)\n",
    "    return train_word2vec_df\n",
    "\n",
    "def load_test_wv_w2v_from_csv(test_X_word2vec_filename):\n",
    "    return pd.read_csv(test_X_word2vec_filename)\n",
    "\n",
    "X_train_wv = load_train_w2v_from_csv(train_X_word2vec_filename)\n",
    "\n",
    "X_train_wv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d0e21cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053024</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024006</td>\n",
       "      <td>0.042881</td>\n",
       "      <td>-0.021136</td>\n",
       "      <td>-0.024381</td>\n",
       "      <td>0.010766</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>-0.056567</td>\n",
       "      <td>0.060535</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012153</td>\n",
       "      <td>-0.030685</td>\n",
       "      <td>0.021339</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>0.032346</td>\n",
       "      <td>-0.013682</td>\n",
       "      <td>-0.020790</td>\n",
       "      <td>-0.050811</td>\n",
       "      <td>0.007726</td>\n",
       "      <td>-0.050212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047366</td>\n",
       "      <td>0.016169</td>\n",
       "      <td>0.018548</td>\n",
       "      <td>0.023616</td>\n",
       "      <td>0.017228</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.020927</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>-0.049627</td>\n",
       "      <td>0.044406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011611</td>\n",
       "      <td>-0.038126</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>-0.016775</td>\n",
       "      <td>0.032452</td>\n",
       "      <td>-0.020109</td>\n",
       "      <td>-0.023841</td>\n",
       "      <td>-0.052119</td>\n",
       "      <td>0.019372</td>\n",
       "      <td>-0.030337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.040361</td>\n",
       "      <td>0.025301</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.043931</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>-0.005833</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>0.012258</td>\n",
       "      <td>-0.065365</td>\n",
       "      <td>0.037350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>-0.035740</td>\n",
       "      <td>0.031067</td>\n",
       "      <td>-0.018500</td>\n",
       "      <td>0.046343</td>\n",
       "      <td>-0.025007</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>-0.047883</td>\n",
       "      <td>0.010271</td>\n",
       "      <td>-0.032912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.053678</td>\n",
       "      <td>0.018155</td>\n",
       "      <td>0.054913</td>\n",
       "      <td>0.037180</td>\n",
       "      <td>0.017946</td>\n",
       "      <td>-0.018485</td>\n",
       "      <td>0.012916</td>\n",
       "      <td>0.031583</td>\n",
       "      <td>-0.064868</td>\n",
       "      <td>0.044907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007292</td>\n",
       "      <td>-0.027373</td>\n",
       "      <td>0.038207</td>\n",
       "      <td>-0.029403</td>\n",
       "      <td>0.048433</td>\n",
       "      <td>-0.023186</td>\n",
       "      <td>-0.020249</td>\n",
       "      <td>-0.044426</td>\n",
       "      <td>-0.002989</td>\n",
       "      <td>-0.037680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049551</td>\n",
       "      <td>-0.002250</td>\n",
       "      <td>0.021974</td>\n",
       "      <td>0.021378</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.006801</td>\n",
       "      <td>0.026745</td>\n",
       "      <td>0.022748</td>\n",
       "      <td>-0.040269</td>\n",
       "      <td>0.015219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005900</td>\n",
       "      <td>-0.026560</td>\n",
       "      <td>0.057077</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.013152</td>\n",
       "      <td>-0.028337</td>\n",
       "      <td>-0.028905</td>\n",
       "      <td>-0.042834</td>\n",
       "      <td>-0.006099</td>\n",
       "      <td>-0.042736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.053024  0.028613  0.024006  0.042881 -0.021136 -0.024381  0.010766   \n",
       "1  0.047366  0.016169  0.018548  0.023616  0.017228  0.000130  0.020927   \n",
       "2  0.040361  0.025301  0.042857  0.043931  0.005333 -0.005833  0.011292   \n",
       "3  0.053678  0.018155  0.054913  0.037180  0.017946 -0.018485  0.012916   \n",
       "4  0.049551 -0.002250  0.021974  0.021378  0.000146 -0.006801  0.026745   \n",
       "\n",
       "          7         8         9  ...       990       991       992       993  \\\n",
       "0  0.003945 -0.056567  0.060535  ... -0.012153 -0.030685  0.021339 -0.007457   \n",
       "1  0.008233 -0.049627  0.044406  ... -0.011611 -0.038126  0.038818 -0.016775   \n",
       "2  0.012258 -0.065365  0.037350  ...  0.013502 -0.035740  0.031067 -0.018500   \n",
       "3  0.031583 -0.064868  0.044907  ... -0.007292 -0.027373  0.038207 -0.029403   \n",
       "4  0.022748 -0.040269  0.015219  ... -0.005900 -0.026560  0.057077  0.003040   \n",
       "\n",
       "        994       995       996       997       998       999  \n",
       "0  0.032346 -0.013682 -0.020790 -0.050811  0.007726 -0.050212  \n",
       "1  0.032452 -0.020109 -0.023841 -0.052119  0.019372 -0.030337  \n",
       "2  0.046343 -0.025007  0.000850 -0.047883  0.010271 -0.032912  \n",
       "3  0.048433 -0.023186 -0.020249 -0.044426 -0.002989 -0.037680  \n",
       "4  0.013152 -0.028337 -0.028905 -0.042834 -0.006099 -0.042736  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_wv = load_train_w2v_from_csv(test_X_word2vec_filename)\n",
    "X_test_wv.head()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f98b6",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3415dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree_word2vec(X_train_wv, Y_train):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    #Initialize the model\n",
    "    clf_decision_word2vec = DecisionTreeClassifier()\n",
    "    # Fit the model\n",
    "    clf_decision_word2vec.fit(X_train_wv, Y_train['target'])\n",
    "    \n",
    "    import joblib\n",
    "    joblib.dump(clf_decision_word2vec, OUTPUT_FOLDER + 'clf_decision_word2vec.pkl')\n",
    "    \n",
    "    return clf_decision_word2vec\n",
    "\n",
    "train_decision_tree_word2vec(X_train_wv, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85d5a2",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc352743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decision_tree_word2vec(Y_test, X_test_wv):\n",
    "    from sklearn.metrics import classification_report\n",
    "    from joblib import load\n",
    "    # Load the model from the file\n",
    "    clf_decision_word2vec = load(\"../model/decision_tree_word2vec.pkl\")\n",
    "    test_predictions_word2vec = clf_decision_word2vec.predict(X_test_wv)\n",
    "\n",
    "    print(classification_report(Y_test['target'], test_predictions_word2vec))\n",
    "    \n",
    "test_decision_tree_word2vec(Y_test, X_test_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd7133",
   "metadata": {},
   "source": [
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with batch size of 100000 of batch  0\n",
      "Training the model with batch size of 100000 of batch  100000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- -0.0001527181448182091\n- -0.00019450907711870968\n- -0.0002684208157006651\n- -0.00027748121647164226\n- -0.0003566486411727965\n- ...\nFeature names seen at fit time, yet now missing:\n- -0.00013550929725170135\n- -0.00016435066936537623\n- -0.00020373801817186177\n- -0.00028594405739568174\n- -0.00034137649345211685\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(clf, OUTPUT_FOLDER \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd_T160_full_batched.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clf\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtrain_sgd_clf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[98], line 12\u001b[0m, in \u001b[0;36mtrain_sgd_clf\u001b[1;34m(Y_train)\u001b[0m\n\u001b[0;32m     10\u001b[0m     train_target \u001b[38;5;241m=\u001b[39m Y_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m][i:i\u001b[38;5;241m+\u001b[39mbatch]\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_wv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     15\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(clf, OUTPUT_FOLDER \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd_T160_full_batched.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:872\u001b[0m, in \u001b[0;36mBaseSGDClassifier.partial_fit\u001b[1;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    860\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    861\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_weight \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    862\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit. In order to use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m weights,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    869\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight)\n\u001b[0;32m    870\u001b[0m         )\n\u001b[1;32m--> 872\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:596\u001b[0m, in \u001b[0;36mBaseSGDClassifier._partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_partial_fit\u001b[39m(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    583\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    593\u001b[0m     intercept_init,\n\u001b[0;32m    594\u001b[0m ):\n\u001b[0;32m    595\u001b[0m     first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 596\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m     n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    608\u001b[0m     _check_partial_fit_first_call(\u001b[38;5;28mself\u001b[39m, classes)\n",
      "File \u001b[1;32mc:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- -0.0001527181448182091\n- -0.00019450907711870968\n- -0.0002684208157006651\n- -0.00027748121647164226\n- -0.0003566486411727965\n- ...\nFeature names seen at fit time, yet now missing:\n- -0.00013550929725170135\n- -0.00016435066936537623\n- -0.00020373801817186177\n- -0.00028594405739568174\n- -0.00034137649345211685\n- ...\n"
     ]
    }
   ],
   "source": [
    "def train_sgd_clf(Y_train):\n",
    "    from sklearn.linear_model import Perceptron\n",
    "    #Initialize the model\n",
    "    clf = Perceptron()\n",
    "    batch = 100000\n",
    "    classes = [-1, 1]\n",
    "    for i in range(0, len(Y_train), batch):\n",
    "        print(\"Training the model with batch size of 100000 of batch \", i)\n",
    "        X_train_wv = pd.read_csv(train_X_word2vec_filename, skiprows=i, nrows=batch, header=1)\n",
    "        train_target = Y_train['target'][i:i+batch]\n",
    "        # Fit the model\n",
    "        clf.partial_fit(X_train_wv, train_target, classes = classes)\n",
    "    \n",
    "    import joblib\n",
    "    joblib.dump(clf, OUTPUT_FOLDER + 'sgd_T160_full_batched.pkl')\n",
    "    \n",
    "    return clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c539c8a",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c831e",
   "metadata": {},
   "source": [
    "### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svc_word2vec(X_train_wv, Y_train):\n",
    "    from sklearn.svm import SVC\n",
    "    #Initialize the model\n",
    "    svm_classifier = SVC()\n",
    "\n",
    "    # Fit the model\n",
    "    svm_classifier.fit(X_train_wv, Y_train['target'])\n",
    "    \n",
    "    import joblib\n",
    "    joblib.dump(svm_classifier, OUTPUT_FOLDER + 'svm_classifier.pkl')\n",
    "    \n",
    "    return svm_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_svc_word2vec(Y_test, X_test_wv):\n",
    "    from sklearn.metrics import classification_report\n",
    "    from joblib import load\n",
    "    svm_classifier = load(OUTPUT_FOLDER + 'svm_classifier.pkl')\n",
    "    reduced_test_features_word2vec = pd.DataFrame(X_test_wv).sample(frac=0.1, random_state=42)\n",
    "    test_predictions_word2vec_svm = svm_classifier.predict(reduced_test_features_word2vec)\n",
    "    print(len(test_predictions_word2vec_svm))\n",
    "    \n",
    "    print(classification_report(pd.Series(Y_test['target']).sample(frac=0.1, random_state=42),test_predictions_word2vec_svm))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95bd86",
   "metadata": {},
   "source": [
    "### with standard scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cddd56",
   "metadata": {},
   "source": [
    "Do not be a retard: low C == less rigid: more spaces for error: train faster\n",
    "\n",
    "C = 1.0 take 4 hours\n",
    "\n",
    "C = 2.0 takes more than 6hours before terminated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cade66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svc_word2vec_general(X_train_wv, Y_train, C, kernel):\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    #Initialize the model\n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel = kernel, C = C))\n",
    "    # Fit the model\n",
    "    clf.fit(X_train_wv, Y_train['target'])\n",
    "    import joblib\n",
    "    joblib.dump(clf, OUTPUT_FOLDER + 'svm_classifier_C0.2_linear.pkl')\n",
    "    \n",
    "    return clf\n",
    "\n",
    "clf = train_svc_word2vec_general(X_train_wv, Y_train, 0.2, 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40832bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_svc_word2vec_general(Y_test, X_test_wv, clf):\n",
    "    from sklearn.metrics import classification_report\n",
    "    # from joblib import load\n",
    "    # clf = load(OUTPUT_FOLDER + 'svm_classifier_scl_linear.pkl')\n",
    "    test_predictions_word2vec_svm_scaled = clf.predict(X_test_wv)\n",
    "\n",
    "    print(classification_report(Y_test['target'], test_predictions_word2vec_svm_scaled))\n",
    "\n",
    "test_svc_word2vec_general(Y_test, X_test_wv, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45acaa",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91664d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_clf(X_train_wv, Y_train):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    #Initialize the model\n",
    "    clf_decision_word2vec = RandomForestClassifier()\n",
    "    # Fit the model\n",
    "    clf_decision_word2vec.fit(X_train_wv, Y_train['target'])\n",
    "    \n",
    "    import joblib\n",
    "    joblib.dump(clf_decision_word2vec, OUTPUT_FOLDER + 'random_forest_dt_clf.pkl')\n",
    "    \n",
    "    return clf_decision_word2vec\n",
    "\n",
    "clf_rfdt = train_random_forest_clf(X_train_wv, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd2ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_forest_clf(Y_test, X_test_wv, clf):\n",
    "    from sklearn.metrics import classification_report\n",
    "    # from joblib import load\n",
    "    # clf = load(OUTPUT_FOLDER + 'svm_classifier_scl_linear.pkl')\n",
    "    test_predictions_word2vec_svm_scaled = clf.predict(X_test_wv)\n",
    "\n",
    "    print(classification_report(Y_test['target'], test_predictions_word2vec_svm_scaled))\n",
    "\n",
    "test_random_forest_clf(Y_test, X_test_wv, clf_rfdt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
