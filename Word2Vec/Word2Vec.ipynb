{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db03b82-98d8-40dd-ae5f-96b4f256536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTPUT_FOLDER = '../model/'\n",
    "\n",
    "from enum import Enum\n",
    "class CellStatus(Enum):\n",
    "    RUN = 1\n",
    "    SKIPPED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6de5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "# Read the CSV file with specified column names\n",
    "df = pd.read_csv(\"../dataset/training.1600000.processed.noemoticon.csv\", \n",
    "                 encoding=\"ISO-8859-1\", names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6aa034",
   "metadata": {},
   "source": [
    "Columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477335fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the original dataset:\n",
      "\n",
      "Index(['target', 'id', 'date', 'flag', 'user', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in the original dataset:\\n\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2a23b",
   "metadata": {},
   "source": [
    "Example of an Row in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689c8807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d146b21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.600000e+06</td>\n",
       "      <td>1.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.998818e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000001e+00</td>\n",
       "      <td>1.935761e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.467810e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.956916e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.002102e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.177059e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.329206e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target            id\n",
       "count  1.600000e+06  1.600000e+06\n",
       "mean   2.000000e+00  1.998818e+09\n",
       "std    2.000001e+00  1.935761e+08\n",
       "min    0.000000e+00  1.467810e+09\n",
       "25%    0.000000e+00  1.956916e+09\n",
       "50%    2.000000e+00  2.002102e+09\n",
       "75%    4.000000e+00  2.177059e+09\n",
       "max    4.000000e+00  2.329206e+09"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9c1c4",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c21871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000001e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target\n",
       "count  1.600000e+06\n",
       "mean   2.000000e+00\n",
       "std    2.000001e+00\n",
       "min    0.000000e+00\n",
       "25%    0.000000e+00\n",
       "50%    2.000000e+00\n",
       "75%    4.000000e+00\n",
       "max    4.000000e+00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = df.dropna()\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "df_cleaned = df_cleaned.drop(columns=[\"date\", \"id\", \"flag\", \"user\"])\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f5b54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b04e21",
   "metadata": {},
   "source": [
    "Remove twitter tag and hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ca08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df_cleaned['text'] = df_cleaned['text'].apply(lambda x: re.sub(r\"http\\S+|@\\S+|#\\S+\", \"\", x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba2ef7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0    - Awww, that's a bummer.  You shoulda got Da...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0   I dived many times for the ball. Managed to s...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0   no, it's not behaving at all. i'm mad. why am..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268dec6",
   "metadata": {},
   "source": [
    "convert target back to -1 0 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ee2f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "-1    800000\n",
       " 1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['target'] = df_cleaned['target'].map({0: -1, 2: 0, 4: 1})\n",
    "\n",
    "df_cleaned['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d13f1",
   "metadata": {},
   "source": [
    "Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb2af120",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_preprocess\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Tokenize the text column to get the new column 'tokenized_text'\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [simple_preprocess(line, deacc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      4\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_preprocess\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Tokenize the text column to get the new column 'tokenized_text'\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43msimple_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeacc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      4\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\gensim\\utils.py:311\u001b[0m, in \u001b[0;36msimple_preprocess\u001b[1;34m(doc, deacc, min_len, max_len)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_preprocess\u001b[39m(doc, deacc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, min_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m):\n\u001b[0;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    Uses :func:`~gensim.utils.tokenize` internally.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 311\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeacc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeacc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m min_len \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_len \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    313\u001b[0m     ]\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\gensim\\utils.py:266\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(text, lowercase, deacc, encoding, errors, to_lower, lower)\u001b[0m\n\u001b[0;32m    264\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deacc:\n\u001b[1;32m--> 266\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mdeaccent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m simple_tokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\gensim\\utils.py:200\u001b[0m, in \u001b[0;36mdeaccent\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    198\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    199\u001b[0m norm \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNFD\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n\u001b[1;32m--> 200\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43municodedata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unicodedata\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNFC\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\Nam\\Documents\\Machine Learning\\NamMLJourney\\.venv\\lib\\site-packages\\gensim\\utils.py:200\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    198\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    199\u001b[0m norm \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNFD\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n\u001b[1;32m--> 200\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ch \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m norm \u001b[38;5;28;01mif\u001b[39;00m unicodedata\u001b[38;5;241m.\u001b[39mcategory(ch) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unicodedata\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNFC\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_cleaned['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df_cleaned['text']]\n",
    "df_cleaned['tokenized_text'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c96433",
   "metadata": {},
   "source": [
    "## Stemming & Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d4957",
   "metadata": {},
   "source": [
    "### PoterStammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2face",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_be_stemmed = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6379b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [awww, that, bummer, you, shoulda, got, david,...\n",
       "1    [is, upset, that, he, can, updat, hi, facebook...\n",
       "2    [dive, mani, time, for, the, ball, manag, to, ...\n",
       "3    [my, whole, bodi, feel, itchi, and, like, it, ...\n",
       "4    [no, it, not, behav, at, all, mad, why, am, he...\n",
       "5                              [not, the, whole, crew]\n",
       "6                                          [need, hug]\n",
       "7    [hei, long, time, no, see, ye, rain, bit, onli...\n",
       "8                         [nope, thei, didn, have, it]\n",
       "9                                     [que, me, muera]\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "df_potter_stemmed = df_to_be_stemmed.copy()\n",
    "# Get the stemmed_tokens\n",
    "df_potter_stemmed['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] \n",
    "                                       for tokens in df_to_be_stemmed['tokenized_text']]\n",
    "df_potter_stemmed['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fc115",
   "metadata": {},
   "source": [
    "### Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0011d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [awww, that, bum, you, should, got, david, car...\n",
       "1    [is, upset, that, he, can, upd, his, facebook,...\n",
       "2    [div, many, tim, for, the, bal, man, to, sav, ...\n",
       "3    [my, whol, body, feel, itchy, and, lik, it, on...\n",
       "4    [no, it, not, behav, at, al, mad, why, am, her...\n",
       "5                               [not, the, whol, crew]\n",
       "6                                           [nee, hug]\n",
       "7    [hey, long, tim, no, see, ye, rain, bit, on, b...\n",
       "8                           [nop, they, didn, hav, it]\n",
       "9                                      [que, me, muer]\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status = CellStatus.SKIPPED\n",
    "df_lancaster_stemmed = df_to_be_stemmed.copy()\n",
    "\n",
    "if status == CellStatus.RUN:\n",
    "    from nltk.stem.lancaster import LancasterStemmer\n",
    "    lancaster_stemmer = LancasterStemmer()\n",
    "    # Get the stemmed_tokens\n",
    "    df_lancaster_stemmed['stemmed_tokens'] = [[lancaster_stemmer.stem(word) for word in tokens] \n",
    "                                            for tokens in df_to_be_stemmed['tokenized_text']]\n",
    "    df_lancaster_stemmed['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424002e9",
   "metadata": {},
   "source": [
    "### Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020d6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [awww, that, bummer, you, shoulda, got, david,...\n",
       "1    [is, upset, that, he, can, updat, his, faceboo...\n",
       "2    [dive, mani, time, for, the, ball, manag, to, ...\n",
       "3    [my, whole, bodi, feel, itchi, and, like, it, ...\n",
       "4    [no, it, not, behav, at, all, mad, whi, am, he...\n",
       "5                              [not, the, whole, crew]\n",
       "6                                          [need, hug]\n",
       "7    [hey, long, time, no, see, yes, rain, bit, onl...\n",
       "8                         [nope, they, didn, have, it]\n",
       "9                                     [que, me, muera]\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status = CellStatus.SKIPPED\n",
    "df_snowball_stemmed = df_to_be_stemmed.copy()\n",
    "\n",
    "if status == CellStatus.RUN:\n",
    "    from nltk.stem.snowball import EnglishStemmer\n",
    "    snowball_stemmer = EnglishStemmer()\n",
    "    # Get the stemmed_tokens\n",
    "    df_snowball_stemmed['stemmed_tokens'] = [[snowball_stemmer.stem(word) for word in tokens] \n",
    "                                            for tokens in df_to_be_stemmed['tokenized_text']]\n",
    "    df_snowball_stemmed['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453e96e",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4ab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [awww, that, bummer, you, shoulda, got, david,...\n",
       "1    [is, upset, that, he, can, update, his, facebo...\n",
       "2    [dived, many, time, for, the, ball, managed, t...\n",
       "3    [my, whole, body, feel, itchy, and, like, it, ...\n",
       "4    [no, it, not, behaving, at, all, mad, why, am,...\n",
       "5                              [not, the, whole, crew]\n",
       "6                                          [need, hug]\n",
       "7    [hey, long, time, no, see, yes, rain, bit, onl...\n",
       "8                         [nope, they, didn, have, it]\n",
       "9                                     [que, me, muera]\n",
       "Name: lemmatized_tokens, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status = CellStatus.SKIPPED\n",
    "df_lemmatized = df_to_be_stemmed.copy()\n",
    "\n",
    "if status == CellStatus.RUN:\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # Get the lemmatized_tokens\n",
    "    df_lemmatized['lemmatized_tokens'] = [[wordnet_lemmatizer.lemmatize(word) for word in tokens] \n",
    "                                          for tokens in df_to_be_stemmed['tokenized_text']]\n",
    "    df_lemmatized['lemmatized_tokens'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90b684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "      <td>[awww, that, bummer, you, shoulda, got, david,...</td>\n",
       "      <td>[awww, that, bummer, you, shoulda, got, david,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[is, upset, that, he, can, update, his, facebo...</td>\n",
       "      <td>[is, upset, that, he, can, update, his, facebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>[dived, many, times, for, the, ball, managed, ...</td>\n",
       "      <td>[dived, many, time, for, the, ball, managed, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[my, whole, body, feel, itchy, and, like, it, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>[no, it, not, behaving, at, all, mad, why, am,...</td>\n",
       "      <td>[no, it, not, behaving, at, all, mad, why, am,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0      -1    - Awww, that's a bummer.  You shoulda got Da...   \n",
       "1      -1  is upset that he can't update his Facebook by ...   \n",
       "2      -1   I dived many times for the ball. Managed to s...   \n",
       "3      -1    my whole body feels itchy and like its on fire    \n",
       "4      -1   no, it's not behaving at all. i'm mad. why am...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [awww, that, bummer, you, shoulda, got, david,...   \n",
       "1  [is, upset, that, he, can, update, his, facebo...   \n",
       "2  [dived, many, times, for, the, ball, managed, ...   \n",
       "3  [my, whole, body, feels, itchy, and, like, its...   \n",
       "4  [no, it, not, behaving, at, all, mad, why, am,...   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0  [awww, that, bummer, you, shoulda, got, david,...  \n",
       "1  [is, upset, that, he, can, update, his, facebo...  \n",
       "2  [dived, many, time, for, the, ball, managed, t...  \n",
       "3  [my, whole, body, feel, itchy, and, like, it, ...  \n",
       "4  [no, it, not, behaving, at, all, mad, why, am,...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemmatized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e0074",
   "metadata": {},
   "source": [
    "## Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643bda16",
   "metadata": {},
   "source": [
    "- Train data ( Subset of data for training ML Model) ~70%\n",
    "- Test data (Subset of data for testing ML Model trained from the train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e99184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train sentiments\n",
      "target\n",
      " 1    560461\n",
      "-1    559539\n",
      "Name: count, dtype: int64\n",
      "Value counts for Test sentiments\n",
      "target\n",
      "-1    240461\n",
      " 1    239539\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "     index                                     stemmed_tokens\n",
      "0  1448643  [and, listen, to, the, song, too, paper, mean,...\n",
      "1  1423081  [think, im, gonna, read, the, half, blood, pri...\n",
      "2  1598349  [just, woke, up, text, emili, goin, out, to, f...\n",
      "3   405940  [wai, to, make, me, jealou, no, bug, in, my, g...\n",
      "4  1050615  [hah, wa, read, and, found, out, he, made, old...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_test(data, sentiment_value_col, tokenised_text_col, test_size=0.3, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split( data[tokenised_text_col],\n",
    "                                                        data[sentiment_value_col], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(df_potter_stemmed, 'target', 'stemmed_tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c2ac1",
   "metadata": {},
   "source": [
    "# Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339bfb7",
   "metadata": {},
   "source": [
    "## Save-gram approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87617e6",
   "metadata": {},
   "source": [
    "### Generate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "# Skip-gram model (sg = 1)\n",
    "vector_size = 1000\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "\n",
    "word2vec_model_file = OUTPUT_FOLDER + 'word2vec_' + str(vector_size) + 'savegram' + '.model'\n",
    "start_time = time.time()\n",
    "stemmed_tokens = pd.Series(df_potter_stemmed['stemmed_tokens']).values\n",
    "# Train the Word2Vec Model\n",
    "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, vector_size = vector_size, workers = workers, window = window, sg = sg)\n",
    "w2v_model.save(word2vec_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d34b8",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d749099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'action':\n",
      "1725\n",
      "212909\n",
      "Length of the vector generated for a word\n",
      "1000\n",
      "Print the length after taking average of all word vectors in a sentence:\n",
      "[ 1.23712711e-01  2.62127500e-02  6.71377555e-02  9.57544819e-02\n",
      " -8.67230294e-04  8.37140810e-03  5.66953830e-02  5.25718667e-02\n",
      " -8.88865665e-02  1.18375942e-01 -3.54103930e-02 -2.81088352e-02\n",
      "  1.31027149e-02  1.21999085e-02  1.03207193e-01 -4.61583920e-02\n",
      " -7.42727965e-02  9.92470421e-03  2.68388987e-02 -2.13900417e-01\n",
      "  7.09337965e-02  1.75605398e-02  1.79810245e-02  2.84867249e-02\n",
      "  8.40547681e-02 -1.13794887e-02  8.05658028e-02 -4.93748225e-02\n",
      " -2.28698328e-01  8.61635581e-02  5.02824001e-02 -9.04745888e-03\n",
      " -1.69876087e-02 -8.65495279e-02  9.35147479e-02  1.80817209e-02\n",
      "  1.08934522e-01  6.13867529e-02 -1.09635241e-01 -1.32616594e-01\n",
      " -1.48112461e-01  6.82336092e-02 -7.32604414e-02  1.31077066e-01\n",
      " -4.97542098e-02 -3.25298794e-02 -1.17841065e-01  8.74581262e-02\n",
      " -1.12966195e-01  4.94225845e-02 -2.34737378e-02 -5.92518412e-02\n",
      "  1.81108993e-02 -7.44959712e-02  2.49145385e-02  1.51455849e-02\n",
      "  1.27240881e-01  9.44742095e-03 -9.19765383e-02 -3.05749085e-02\n",
      " -8.53730515e-02  3.07752527e-02 -2.42137704e-02 -5.41621074e-02\n",
      " -3.81557806e-03  3.94857787e-02 -7.11645633e-02  1.38528392e-01\n",
      "  5.37661910e-02 -2.58663166e-02  9.26191881e-02  2.99788010e-03\n",
      "  6.55801371e-02 -5.16976789e-02  4.07856377e-03  8.79295692e-02\n",
      "  1.74654592e-02 -1.80783514e-02  2.69778399e-03 -1.41594280e-02\n",
      " -4.75270953e-03  3.15715186e-02 -3.45848389e-02  1.20498776e-01\n",
      " -2.43045956e-01  1.12073822e-02  1.78929530e-02  1.05074406e-01\n",
      "  4.10324670e-02  1.15531199e-01  3.66990305e-02  3.02166808e-02\n",
      " -2.22635139e-02  8.19876045e-02  1.90227896e-01  1.58111919e-02\n",
      "  1.53463885e-01 -1.65432006e-01  1.60378609e-02  5.03526144e-02\n",
      " -6.82246983e-02  1.60516039e-01  5.27947024e-02  4.03090827e-02\n",
      "  5.18257320e-02  2.85484456e-02  1.43803963e-02  7.36703649e-02\n",
      " -6.23490708e-03 -1.44690629e-02 -2.57528815e-02 -6.27573654e-02\n",
      "  4.73709665e-02  4.66482043e-02  8.19230303e-02 -1.47670778e-02\n",
      "  5.11105731e-02 -4.62552868e-02 -4.18308415e-02 -1.36434227e-01\n",
      " -3.89675498e-02  5.22646867e-02  4.42556441e-02  6.31306097e-02\n",
      " -9.09917727e-02  1.69110857e-02 -1.68514967e-01  3.19212265e-02\n",
      " -2.10804809e-02  3.28183882e-02  8.95239040e-02  8.86994749e-02\n",
      " -1.16656296e-01 -2.75044236e-02  4.98946337e-03  3.05666064e-04\n",
      " -1.05179965e-01 -3.26105282e-02 -9.07264724e-02 -1.50828585e-01\n",
      "  1.09102421e-01 -4.89364937e-02 -1.52197042e-02 -1.85839571e-02\n",
      " -4.68792673e-03 -2.25611068e-02  7.75306076e-02  4.72085588e-02\n",
      " -3.95757966e-02  2.08075927e-03  1.22834995e-01 -1.01541199e-01\n",
      " -2.77112518e-02  1.86754063e-01 -2.53045857e-01  6.13679253e-02\n",
      "  1.68052226e-01 -2.94582862e-02 -4.93491106e-02  9.02605578e-02\n",
      "  2.86921374e-02  9.59314182e-02 -1.97080612e-01 -1.88724071e-01\n",
      " -1.12777904e-01  2.30406791e-01  1.23550281e-01  5.72673790e-02\n",
      " -3.20055149e-02  1.03275709e-01 -3.75838876e-02  7.60593936e-02\n",
      "  4.20936896e-03 -3.87211703e-02 -8.75463560e-02 -3.28808390e-02\n",
      " -4.67939489e-02  3.03741656e-02  4.29613218e-02  8.49052891e-02\n",
      "  1.10292949e-01  8.55667666e-02  1.13658950e-01  8.84113535e-02\n",
      " -3.38687301e-02 -6.76982254e-02 -2.85168421e-02 -1.96249466e-02\n",
      " -5.09643592e-02  1.18486304e-02  1.47596046e-01 -1.12682089e-01\n",
      "  7.62059316e-02 -4.74062786e-02  1.05289884e-01 -8.50045085e-02\n",
      "  1.01447860e-02 -6.01055920e-02  6.89986497e-02 -6.08641282e-02\n",
      " -5.32924049e-02  1.20338639e-02  1.90106124e-01 -9.49889608e-03\n",
      "  4.61853482e-02 -3.88694890e-02 -1.37092397e-01 -7.98794478e-02\n",
      " -5.96509613e-02  5.37091568e-02  6.42941743e-02 -3.59506421e-02\n",
      " -8.47489461e-02  4.61439863e-02  2.81986464e-02 -1.90177158e-01\n",
      " -3.14016975e-02 -2.04604119e-04 -1.95605725e-01 -4.30787578e-02\n",
      "  3.23386490e-02  1.22070000e-01  1.57224402e-01  2.81808507e-02\n",
      " -3.19879018e-02 -5.77280894e-02  6.53242394e-02 -6.13789968e-02\n",
      "  5.82944937e-02  3.39116603e-02  9.23342537e-03 -1.14457868e-01\n",
      "  7.61012882e-02  1.23994604e-01 -7.02964291e-02 -1.12903416e-02\n",
      "  7.01451078e-02 -1.34795293e-01  1.17438041e-01 -5.96815273e-02\n",
      " -8.74388963e-02  7.21660107e-02 -8.56762454e-02  6.33334666e-02\n",
      "  1.05183624e-01  9.79046673e-02  8.38976130e-02 -7.99035653e-03\n",
      "  3.10516334e-03 -1.65805761e-02  6.77904710e-02 -6.27820343e-02\n",
      " -1.29272372e-01 -2.42895205e-02 -1.83684379e-02 -6.50560623e-03\n",
      "  1.12659916e-01 -2.93197874e-02 -1.04679108e-01  5.25528193e-02\n",
      " -1.86409950e-01  1.28508404e-01 -4.98489328e-02 -3.38011831e-02\n",
      " -2.28458233e-02  4.58993874e-02 -1.24708880e-02  2.06862062e-01\n",
      " -1.87464789e-01  4.10981961e-02 -9.77920070e-02  8.46779495e-02\n",
      "  9.34476964e-03  3.82161625e-02 -3.98926325e-02  1.66120157e-01\n",
      "  2.92718243e-02 -9.18295756e-02 -5.82822859e-02  1.09449044e-01\n",
      "  1.59974676e-02  2.34472044e-02 -7.99167231e-02  2.35110857e-02\n",
      "  2.21852794e-01  1.46121830e-02  5.36068343e-02 -5.29092960e-02\n",
      " -4.58585061e-02  7.51210898e-02 -8.17060322e-02  9.48434621e-02\n",
      "  1.11966491e-01  1.52890440e-02  4.18132246e-02 -7.08474442e-02\n",
      "  1.45810479e-02  1.52100315e-02  2.86117028e-02 -7.04572052e-02\n",
      " -1.29661798e-01  1.48768038e-01 -2.42702868e-02 -1.34222791e-01\n",
      " -2.12315284e-02 -9.92825776e-02 -2.19853260e-02  5.77597208e-02\n",
      "  7.78564662e-02 -1.26801029e-01  2.01160479e-02 -6.05037808e-02\n",
      "  1.70844160e-02  1.07801119e-02  3.93323787e-02  2.08577514e-02\n",
      "  2.20001899e-02 -1.22850239e-01  2.97135990e-02 -1.31341815e-01\n",
      " -2.09403690e-02  6.98452815e-02  6.70713037e-02 -1.40656620e-01\n",
      " -1.17594182e-01 -4.32490818e-02 -3.81326415e-02 -7.09595084e-02\n",
      "  8.85425434e-02 -5.94156012e-02  5.75641058e-02  1.02779642e-01\n",
      "  5.03501855e-02  1.19533110e-02 -4.83665103e-03 -1.21929444e-01\n",
      "  2.00389903e-02  1.92154236e-02 -8.46585631e-02 -4.45565581e-02\n",
      " -5.21219801e-03  9.71760601e-02 -7.05377534e-02 -1.21594325e-01\n",
      " -9.05685797e-02  3.77329588e-02 -2.36330837e-01 -5.72709553e-02\n",
      " -1.82888675e-02  1.51004251e-02 -3.41958106e-02 -1.32849370e-03\n",
      " -5.15603349e-02  1.53748751e-01 -5.77565394e-02  2.14778692e-01\n",
      " -1.05651982e-01 -8.77434239e-02  7.52795935e-02 -1.79333147e-02\n",
      " -1.21578753e-01  1.94932390e-02 -9.17319283e-02  1.33885061e-02\n",
      "  6.60180720e-03 -5.61723784e-02  1.24225141e-02 -3.58917899e-02\n",
      " -1.87322706e-01 -5.80264963e-02 -6.02183864e-02 -9.94741693e-02\n",
      " -1.06380507e-01  3.01054176e-02 -6.32390007e-02  7.79568553e-02\n",
      "  4.70282361e-02 -1.20899063e-02  7.44614750e-02  9.47831124e-02\n",
      " -2.40069274e-02  1.13644801e-01  1.38709262e-01  6.72818795e-02\n",
      "  1.84577107e-02  3.48951146e-02  3.74584384e-02  1.03040196e-01\n",
      "  1.08421341e-01  3.44754644e-02 -4.50838357e-04 -1.36173159e-01\n",
      " -1.04376197e-01 -2.60991026e-02  8.10491815e-02  6.02657907e-02\n",
      "  7.62553290e-02  5.87176494e-02 -9.78789330e-02  4.95779440e-02\n",
      " -9.06543899e-03  1.05791263e-01 -4.48869262e-03 -9.16313529e-02\n",
      "  3.25602368e-02 -1.00032821e-01  3.91319878e-02  1.22125380e-01\n",
      " -6.67123497e-02  5.36516421e-02 -1.01280488e-01 -1.16664305e-01\n",
      " -9.85958576e-02 -3.75724249e-02  1.06501142e-02 -5.67785278e-02\n",
      "  3.21775465e-03  6.60582557e-02  1.33626059e-01 -8.05021003e-02\n",
      " -4.07993086e-02 -1.34293944e-01  8.89827497e-03  2.85054501e-02\n",
      " -3.83239775e-03 -3.03681102e-02 -5.98439462e-02  8.20314363e-02\n",
      " -1.91847950e-01 -6.67732283e-02 -4.65558730e-02 -1.52395731e-02\n",
      " -4.61690947e-02  5.43588512e-02  9.52656269e-02  3.54720019e-02\n",
      "  5.79543971e-02  7.40996450e-02  2.59767529e-02  8.27532485e-02\n",
      "  8.54203701e-02  5.11293076e-02  3.41903535e-03  7.28972778e-02\n",
      "  2.97350556e-01 -5.32212183e-02  1.43205270e-01 -5.96922301e-02\n",
      "  2.48733554e-02 -4.50279145e-03  9.48061198e-02  5.12237735e-02\n",
      " -8.10937211e-02 -1.77670389e-01 -6.13740124e-02  1.36800604e-02\n",
      " -7.39579126e-02 -1.92035869e-01  1.48237661e-01 -2.15829490e-03\n",
      "  1.06284343e-01  1.31186351e-01  1.86316408e-02  3.87300923e-02\n",
      "  1.84703041e-02 -1.46747977e-02 -6.33754879e-02 -1.27664462e-01\n",
      " -2.03989893e-02  3.84866670e-02 -9.00102481e-02 -3.36373821e-02\n",
      "  9.55233946e-02 -2.28791796e-02  4.58528996e-02  1.17073610e-01\n",
      "  8.46421495e-02 -1.00976057e-01 -1.08877659e-01 -2.70702653e-02\n",
      " -1.19717106e-01 -2.11397424e-01 -7.46180024e-03  1.06410563e-01\n",
      " -6.78260177e-02 -2.06296127e-02  1.27782866e-01  2.13235065e-01\n",
      "  2.16072965e-02 -4.04325649e-02 -1.95716619e-02 -7.18566310e-03\n",
      "  6.65311217e-02 -4.30765711e-02 -5.73832318e-02 -4.57074940e-02\n",
      "  2.96098553e-02 -1.16550699e-01 -4.92165424e-02  5.49512804e-02\n",
      " -1.89628899e-02  1.47535011e-01  1.14759423e-01  3.35856341e-02\n",
      " -4.21982147e-02 -1.38352901e-01  1.18961520e-01  1.89965740e-01\n",
      "  1.06518015e-01 -4.83015226e-03 -8.88038129e-02  4.21901233e-02\n",
      " -2.69835331e-02 -3.65158804e-02 -6.18502460e-02 -2.37514183e-01\n",
      " -4.24309038e-02 -9.51085240e-02 -6.35907724e-02  1.95119709e-01\n",
      " -2.96109598e-02  8.02835450e-02  1.56652436e-01  6.85472563e-02\n",
      "  4.44870219e-02  1.16876617e-01 -6.78765252e-02  9.32046771e-02\n",
      " -3.46050113e-02 -2.09060118e-01 -1.35122044e-02  9.16745067e-02\n",
      "  3.00354566e-02  1.78728495e-02  3.08917016e-02  1.49629042e-01\n",
      " -1.12563148e-01 -1.22346297e-01  8.00020620e-03 -3.90162133e-02\n",
      "  7.65607599e-03 -2.11031865e-02 -4.43284288e-02 -9.72484499e-02\n",
      " -9.96860489e-02 -1.63422197e-01 -3.20890136e-02 -7.89874122e-02\n",
      " -1.87361185e-02  2.67757736e-02 -6.81062788e-03 -2.79676728e-03\n",
      "  1.73204299e-02 -1.05025090e-01  6.02785219e-03 -7.44584277e-02\n",
      "  7.66352490e-02  7.93557614e-02  8.25860649e-02 -2.60895099e-02\n",
      "  1.46348432e-01  2.37588715e-02  1.10178001e-01  5.13490960e-02\n",
      " -7.88949206e-02  1.35090891e-02 -5.96776903e-02 -1.09478839e-01\n",
      "  5.71241789e-02 -2.89289057e-02 -1.43107072e-01  6.56853570e-03\n",
      " -4.70663942e-02 -2.53826976e-02  8.94378573e-02  1.79633334e-01\n",
      " -6.73906133e-02 -2.98046861e-02  4.30777706e-02  1.31639928e-01\n",
      "  1.57228988e-02 -5.52544408e-02 -2.64836270e-02  3.96697707e-02\n",
      " -3.44955996e-02  7.78719336e-02  4.82755341e-02  3.81182358e-02\n",
      "  2.95545347e-02  1.07609488e-01 -2.53440812e-03 -3.23715769e-02\n",
      " -1.36874184e-01 -1.71530433e-02  2.34914292e-02  7.53225908e-02\n",
      "  7.13535165e-03  6.25786334e-02 -9.31311399e-04  3.50559391e-02\n",
      " -1.23076160e-02  1.48047004e-02 -8.54715779e-02  4.62642871e-02\n",
      "  1.02585472e-01 -6.53900877e-02 -7.15844780e-02  6.95458800e-02\n",
      " -1.19266741e-01  5.07353134e-02 -4.35670428e-02 -6.30239844e-02\n",
      " -4.39086594e-02  5.35556162e-03 -1.13688245e-01 -2.42235195e-02\n",
      " -4.97080423e-02 -6.19886257e-02 -1.47459581e-02  6.96236864e-02\n",
      " -2.06690375e-02 -4.92409579e-02 -3.55175994e-02 -6.97846562e-02\n",
      " -7.01995194e-02  5.23956008e-02 -8.97274613e-02 -5.06455787e-02\n",
      " -7.13024065e-02 -6.32721707e-02  3.20848152e-02  7.19707832e-02\n",
      " -3.19096483e-02 -1.07032835e-01 -3.55147710e-03  9.05179158e-02\n",
      "  4.64108847e-02 -5.07133938e-02 -4.92047705e-02 -1.19441830e-01\n",
      " -1.57341361e-02 -8.30544904e-02  8.78929943e-02 -4.24508452e-02\n",
      "  7.59798959e-02  8.29745531e-02 -6.63282275e-02 -1.15150556e-01\n",
      "  4.20046523e-02  4.34645452e-02 -1.58376601e-02  6.69018030e-02\n",
      "  2.04914846e-02  8.23555663e-02  1.15264161e-02  1.59591325e-02\n",
      " -3.70548144e-02 -6.51695132e-02 -2.75543965e-02 -5.31829186e-02\n",
      "  1.03766330e-01 -1.04329489e-01  1.98237095e-02 -2.93582701e-03\n",
      " -3.46531495e-02  8.22478905e-02  1.63069945e-02  5.82092665e-02\n",
      "  6.41873851e-02 -5.39603755e-02  1.30282072e-02  1.01483978e-01\n",
      " -2.63352189e-02  1.29098341e-01  6.02429593e-03 -7.62190074e-02\n",
      "  1.06211364e-01  1.18307639e-02 -4.92220707e-02  4.81954589e-02\n",
      " -4.69431803e-02 -2.36970466e-02 -2.37697065e-02  2.21091751e-02\n",
      " -3.28951329e-03 -1.42605945e-01 -1.67736247e-01  6.56970292e-02\n",
      " -1.18714072e-01  8.10371619e-03  2.83931345e-02 -6.15766868e-02\n",
      "  2.50744466e-02  8.00084546e-02  5.14690354e-02  5.47196344e-02\n",
      "  2.55409139e-03  1.35939168e-02  1.07589491e-01  1.06710322e-01\n",
      "  9.02350468e-04 -4.76303510e-02  3.07358056e-02 -5.11617437e-02\n",
      " -4.39955182e-02  1.24894597e-01  1.25946209e-01 -2.59606633e-02\n",
      " -2.33085856e-01 -6.45865873e-02  1.48202116e-02 -1.55557618e-01\n",
      " -1.39351040e-01 -6.05515912e-02 -3.63770761e-02 -9.36533976e-03\n",
      " -3.53996865e-02 -2.26698201e-02  1.05525389e-01 -6.15136698e-02\n",
      "  1.15419142e-01  1.82400540e-01  1.99815538e-02  7.15286657e-02\n",
      "  6.06089123e-02 -9.07444432e-02  9.53943804e-02 -8.75226259e-02\n",
      "  3.53085436e-02 -3.27459946e-02 -2.97224279e-02  2.49223914e-02\n",
      " -3.61494310e-02 -1.22786187e-01  3.47009301e-02  5.38037159e-02\n",
      " -6.05617240e-02 -1.25039205e-01  1.47319078e-01 -2.64095277e-01\n",
      " -1.80987865e-02 -1.25262916e-01 -1.21231154e-01  8.85340348e-02\n",
      " -1.62425891e-01 -1.25236928e-01 -8.39913934e-02  2.83160638e-02\n",
      "  1.51810735e-01  7.15765357e-02  2.01390367e-02 -1.57314748e-01\n",
      "  1.43586472e-01 -8.84271935e-02 -6.99821934e-02  8.09882209e-02\n",
      " -2.22496390e-02  2.26800308e-01 -1.12504654e-01  5.51065505e-02\n",
      " -6.09812997e-02 -3.13369818e-02 -1.58016190e-01  6.37574419e-02\n",
      "  7.70186782e-02 -3.47998217e-02 -1.88987941e-01  7.51635805e-02\n",
      "  4.87201437e-02 -9.54656228e-02 -1.38092369e-01  4.36622128e-02\n",
      "  1.24860801e-01 -1.15019903e-01 -7.38751814e-02 -6.30352944e-02\n",
      " -1.13215320e-01 -8.07053670e-02 -6.36471435e-02 -2.57884469e-02\n",
      "  2.01339610e-02  8.51304550e-03  4.28463481e-02 -1.23276047e-01\n",
      "  1.26861617e-01 -9.38675553e-02 -2.80060489e-02  4.72203791e-02\n",
      "  8.76552537e-02 -1.30814567e-01 -4.63793287e-03  9.63033885e-02\n",
      " -5.77250607e-02 -1.96269557e-01  2.13232618e-02  4.67095785e-02\n",
      "  8.23765621e-02 -1.23798676e-01 -1.23807257e-02 -1.06508717e-01\n",
      " -6.25900850e-02 -1.36803061e-01  5.56741953e-02  3.87696102e-02\n",
      "  1.40311062e-01 -2.47740336e-02 -7.25358129e-02 -4.92179673e-03\n",
      " -7.98963755e-02  1.18028320e-01  1.00770853e-01 -5.49995713e-02\n",
      "  3.69336307e-02  4.11215983e-02  2.26879981e-03 -1.11722350e-02\n",
      " -6.67413771e-02 -7.41951913e-02  6.46484122e-02 -1.23196263e-02\n",
      "  1.15430295e-01 -9.16967168e-02  4.66445945e-02 -1.01354383e-01\n",
      "  6.07931279e-02 -3.60646769e-02 -1.13548068e-02 -5.74192107e-02\n",
      " -2.63549406e-02  2.62672398e-02  4.82400991e-02 -6.56536669e-02\n",
      "  2.93224417e-02  6.18246384e-02 -5.23632243e-02  4.05755639e-02\n",
      "  1.61523242e-02  9.66956168e-02 -6.71157986e-03 -1.15878716e-01\n",
      " -1.69239536e-01 -5.12196794e-02  2.19096377e-01 -7.34565035e-02\n",
      "  2.73408424e-02 -9.63074192e-02 -1.18751943e-01  3.28182802e-02\n",
      "  2.13682745e-02  8.01127702e-02  1.35421017e-02  6.01299778e-02\n",
      " -7.49986246e-02  5.90156652e-02  2.13266518e-02 -7.52305239e-02\n",
      " -6.83843195e-02 -3.28269345e-03 -6.16278425e-02 -8.88532680e-03\n",
      "  8.72451738e-02  4.92882803e-02  9.64321643e-02  4.04262431e-02\n",
      " -3.88078354e-02 -6.30902722e-02  2.25433870e-03  8.81064311e-02\n",
      " -7.47806355e-02  8.06587413e-02  5.29324077e-02 -7.10654333e-02\n",
      "  7.63090625e-02 -4.01721075e-02 -2.64773704e-02 -8.79188180e-02\n",
      " -3.85374203e-02  3.21662612e-02 -1.08701713e-01 -5.10836169e-02\n",
      "  2.30916198e-02 -8.57415050e-02  9.25306231e-02  3.79548483e-02\n",
      "  3.29782628e-02 -7.61013776e-02 -5.92212491e-02  3.79315391e-02\n",
      "  2.89880913e-02 -2.68572699e-02 -5.37346788e-02  4.47181985e-02\n",
      " -5.58444932e-02  8.67771655e-02  5.53848483e-02  1.89393163e-01\n",
      " -8.27280357e-02 -1.32106589e-02  9.82036591e-02  9.41719413e-02\n",
      " -1.34269848e-01 -6.74103945e-02 -1.66278884e-01 -8.86667743e-02\n",
      " -1.18709132e-01 -2.23382320e-02 -6.76174015e-02  3.11072804e-02\n",
      "  2.61397008e-02 -3.86420302e-02 -5.74457757e-02 -1.16743870e-01\n",
      " -1.30480677e-01 -2.07989905e-02  5.92026040e-02  5.11647463e-02\n",
      "  1.15441799e-01 -1.74352989e-01  2.45567076e-02  2.83787809e-02\n",
      " -1.10598676e-01  1.44301742e-01 -4.97533083e-02 -9.17038471e-02\n",
      " -4.93133701e-02  1.32828042e-01 -8.20123628e-02 -5.77421412e-02\n",
      "  5.18619604e-02 -1.92052409e-01  8.88556466e-02 -6.78025410e-02\n",
      " -2.04415023e-02  7.61517659e-02 -1.93704396e-01  2.41129827e-02\n",
      " -8.40800852e-02 -7.86286369e-02 -1.06694870e-01  1.55248335e-02\n",
      "  3.26122269e-02  6.22909889e-03  1.17940292e-01 -2.86120065e-02\n",
      " -6.46480843e-02  8.45678989e-03  2.12550703e-02  1.47436738e-01\n",
      " -1.78823709e-01 -1.48399854e-02 -3.46963406e-02  6.60457760e-02\n",
      "  2.95195282e-02 -6.83414787e-02  4.17109244e-02  7.00422600e-02\n",
      " -4.48829867e-02  3.72339152e-02 -6.35701492e-02 -7.24803135e-02\n",
      "  8.07279870e-02  3.71025205e-02  6.20330544e-03 -7.04085305e-02\n",
      " -1.36834294e-01 -7.71683262e-05  1.25578577e-02 -1.66232154e-01\n",
      "  9.56443977e-03 -1.35615142e-02  4.30085044e-03  9.62032098e-03\n",
      "  2.15065815e-02 -1.33289829e-01  1.62666842e-01 -2.68831663e-02\n",
      "  2.88654566e-02  1.34878486e-01  2.33765040e-02 -5.32333776e-02\n",
      "  5.54878339e-02 -8.75032172e-02 -9.09974352e-02  7.41950050e-02\n",
      "  8.42293352e-02  9.97095481e-02 -4.92861215e-03  4.72255275e-02\n",
      "  3.56935635e-02  1.24706917e-01 -2.49775182e-02 -1.26090750e-01\n",
      "  8.13852027e-02 -5.44377379e-02  9.22633186e-02 -7.27436095e-02\n",
      " -4.86120991e-02 -5.74476980e-02 -4.45091585e-03 -1.47464618e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Load the model from the model file\n",
    "sg_w2v_model = Word2Vec.load(word2vec_model_file)\n",
    "sg_w2v_model_wv = sg_w2v_model.wv\n",
    "# Unique ID of the word\n",
    "print(\"Index of the word 'action':\")\n",
    "print(sg_w2v_model_wv.key_to_index[\"action\"])\n",
    "# Total number of the words \n",
    "print(len(sg_w2v_model_wv.key_to_index))\n",
    "# Print the size of the word2vec vector for one word\n",
    "print(\"Length of the vector generated for a word\")\n",
    "print(len(sg_w2v_model_wv['action']))\n",
    "# Get the mean for the vectors for an example review\n",
    "print(\"Print the length after taking average of all word vectors in a sentence:\")\n",
    "print(np.mean([sg_w2v_model_wv[token] for token in df_potter_stemmed['stemmed_tokens'][0]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7ef6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_FOLDER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word2vec_filename \u001b[38;5;241m=\u001b[39m \u001b[43mOUTPUT_FOLDER\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_review_word2vec_sg.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(word2vec_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m word2vec_file:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39miterrows():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OUTPUT_FOLDER' is not defined"
     ]
    }
   ],
   "source": [
    "word2vec_filename = OUTPUT_FOLDER + 'train_review_word2vec_sg.csv'\n",
    "with open(word2vec_filename, 'w+') as word2vec_file:\n",
    "    for index, row in X_train.iterrows():\n",
    "        model_vector = (np.mean([sg_w2v_model_wv[token] for token in row['stemmed_tokens']], axis=0)).tolist()\n",
    "        if index == 0:\n",
    "            header = \",\".join(str(ele) for ele in range(1000))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "        # Check if the line exists else it is vector of zeros\n",
    "        if type(model_vector) is list:  \n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        else:\n",
    "            line1 = \",\".join([str(0) for i in range(1000)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
